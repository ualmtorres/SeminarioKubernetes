////
NO CAMBIAR!!
Codificación, idioma, tabla de contenidos, tipo de documento
////
:encoding: utf-8
:lang: es
:toc: right
:toc-title: Tabla de contenidos
:doctype: book
:imagesdir: ./images
:icons: font


////
Nombre y título del trabajo
////
# Kubernetes
Cloud-DI Team - Departamento de Informática. UAL

image::di.png[]

// NO CAMBIAR!! (Entrar en modo no numerado de apartados)
:numbered!: 

[abstract]
== Resumen
////
COLOCA A CONTINUACION EL RESUMEN
////

[IMPORTANT]
====
*TRABAJO EN CURSO*
====

////
COLOCA A CONTINUACION LOS OBJETIVOS
////
.Objetivos
* Conocer la importancia del uso de Kubernetes en el ciclo de vida de las aplicaciones actuales.
* Introducir los conceptos básicos de Kubernetes.
* Usar Minikube para el desarrollo y prueba en local.
* Usar `kubectl` para la administración básica de Kubernetes.
* Escalar aplicaciones.
* Realizar actualizaciones en caliente (_rolling updates_).
* Desplegar aplicaciones con archivos YAML.
* Aprender a usar Kubernetes Dashboard.
    
[TIP]
====
Disponibles los repositorios usados en este seminario:

* https://github.com/ualmtorres/json-producer[Código fuente de JSON Producer]
* https://github.com/ualmtorres/json-reader[Código fuente de JSON Reader]
* https://github.com/ualmtorres/jsonproducerreader[Archivos YAML de JSON Producer y Reader]
====
// Entrar en modo numerado de apartados
:numbered:

## Introducción

La adopción de Docker sigue creciendo de forma imparable y cada vez más organizaciones lo usan en producción. Por tanto, se hace necesario contar con una plataforma de orquestación que permita administrar y escalar los contenedores.

Supongamos que hemos comenzado a usar Docker y hemos hecho un despliegue de un par de servidores. De pronto, la aplicación comienza a tener un gran tráfico de entrada y hay que escalar a una gran cantidad de servidores para atender la demanda. Aquí es donde entra Kubernetes para hacer tareas del tipo _dónde debe ir un contenedor, cómo se monitorizan esos contenedores_ o _cómo se reinician cuando tengan un problema_.

## Conceptos básicos

Kubernetes es una plataforma de código abierto para despliegue automático, escalado y gestión de aplicaciones contenedorizadas. 

[quote,Documentación oficial de Kubernetes (https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)]
____
Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.

The name Kubernetes originates from Greek, meaning helmsman or pilot. Google open-sourced the Kubernetes project in 2014. Kubernetes builds upon a decade and a half of experience that Google has with running production workloads at scale, combined with best-of-breed ideas and practices from the community.


____
Kubernetes ofrece una abstracción en la que permite el despliegue de aplicaciones en un cluster sin pensar en las máquinas que lo soportan. 

### Cluster de Kubernetes

Un cluster de Kubernetes está formado por dos tipos de recursos:

* El *Master* coordina el cluster. Coordina todas las actividades del cluster como organizar (schedule) las aplicaciones, mantener el estado deseado de las aplicaciones, escalado, despliegue de actualizaciones, y demás. También recoge información de los nodos worker y Pods.
* Los *Nodos* son _workers_ que ejecutan las aplicaciones. Cada nodo contiene un agente denominado _Kubelet_ que gestiona el nodo y mantiene la comunicación con el Máster. El nodo también tiene herramientas para trabajar con contenedores, como Docker.

[NOTE]
====
Un cluster Kubernetes en producción debería tener al menos 3 nodos. En entornos de producción se usan varios nodos máster para que los clusters sean tolerantes a fallos y ofrezcan alta disponibilidad.
====

image::KubernetesCluster.svg[]

Al desplegar una aplicación en Kubernetes el Master inicia los contenedores de la aplicación. El máster organiza los contenedores para que se ejecuten en los nodos del cluster. Los nodos se comunican con el master usando la https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#-strong-api-overview-strong-[API de Kubernetes]. La API es expuesta a través del nodo Master y es posible usarla directamente para intectuar con el cluster.

.Lista de pods usando la API de Kubernetes
====
[source, bash]
----

$ curl http://<kubernetes_home>/api/v1/namespaces/default/pods
----

[source, json]
----
{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces/default/pods",
    "resourceVersion": "10803"
  },
  "items": [
    {
      "metadata": {
        "name": "hello-minikube-64c7df9db-ffwtn",
        "generateName": "hello-minikube-64c7df9db-",
        "namespace": "default",
        "selfLink": "/api/v1/namespaces/default/pods/hello-minikube-64c7df9db-ffwtn",
        "uid": "652c298a-6dc2-4aec-a72f-390669fed6d2",
        "resourceVersion": "10608",
        "creationTimestamp": "2019-07-08T18:02:23Z",
        "labels": {
          "pod-template-hash": "64c7df9db",
          "run": "hello-minikube"
        },
....
----
====

Los clusters de Kubernetes se pueden desplegar sobre máquinas físicas o virtuales. Para comenzar a practicar con Kubernetes o para tareas de desarrollo, https://github.com/kubernetes/minikube[Minikube] es una buena opción. En la sección <<Minikube>> se presenta más información sobre esta plataforma. Minikube está disponible para Windows, Linux y MacOS.

### Arquitectura de Kubernetes

Tal y como hemos introducido en el apartado anterior, un cluster de Kubernetes está formado por dos tipos de unidades, el nodo _Master_ y los nodos _Worker_ (o siemplemente _Nodos_).

La figura siguiente ilustra estas dos unidades, así como algunos de los componentes más importantes en su interior.

image::KubernetesArchitecture.png[]

* Plugins de red: Permite la conexión entre pods de nodos diferentes y la integración de soluciones de red diferentes (overlay, L3, ...)
* `etcd`: una base de datos clave-valor donde Kubernetes guarda todos los datos del cluster.
* API server: Componente del Master que expone la API de Kubernetes. Es el front-end del plano de control de Kubernetes.
* Control Manager: Se encarga de comprobar si el estado deseado coincide con la realidad (p.e. número de réplicas)
* Scheduler: Componente del master que observa qué pods se han creado nuevos y no tienen nodo asignado, y les selecciona un nodo donde se puedan ejecutar.
* `kubelet`: Agente que se se ejecuta en cada nodo worker del cluster y que asegura que los nodos están en ejecución y sanos. *`kubelet` no gestiona los pods que no han sido creados por Kubernetes.* 
* `kube-proxy`: Mantiene las reglas de networking en los nodos para los pods que se ejecutan en él de acuerdo con las especificaciones de los manifiestos.
* `cAdvisor`: Recoge datos de uso de los contenedores.
* Plano de control o _Control plane_: Nivel de orquestación de contenedores que expone la API para definir, desplegar y gestionar el ciclo de vida de los contenedores.
* Plano de datos o _Data Plane_: Nivel que proporciona los recursos, como CPU. memoria, red y almacenamiento, para que los pods se puedan ejecutar y conectar a la red.

[TIP]
====
https://etcd.io/[etcd], es una base de datos clave-valor fiable y distribiuda para los datos más críticos de un un sistema distribuido. Dado que Kubernetes guarda todos los datos del cluster en ella, se deberían mantener copias de seguridad de esta base de datos y disponer de un plan de recuperación ante posibles desastres.
====

[NOTE]
====
Los componentes `kube-proxy`, `kube-scheduler`, `kube-controller-manager`, `etcd`, `kubelet`, así como los componentes de red se eejcutan como contenedores en cada uno de los nodos del cluster de Kubernetes. Basta con abrir un terminal en uno de los nodos del cluster y comprobarlo. Si lo hacemos, veremos como en los nodos worker están los contenedores de los componentes de Kubernetes junto con los contenedores de las aplicaciones que se están ejecutando en el nodo.

Un ejercicio interesante es detener el contenedor `kubelet` y ver cómo el nodo pasa a estar inactivo. En caso de ser el único nodo de trabajo, los contenedores de los nuevos despliegues quedarán en el estado `Pending` mientras `kubelet` no vuelva a estar disponible.
====
### Objetos de Kubernetes

Kubernetes ofrece una serie de objetos básicos y una serie de abstracciones de nivel superior llamadas Controladores. Los Controladores se basan en los objetos básicos y proporcionan funcionalidades adicionales sobre los objetos básicos

Los objetos básicos de Kubernetes son:

* Pod
* Service
* Volume
* Namespace

Los objetos de nivel superior o Controladores se basan en los objetos básicos y ofrecen funcionalidades adicionales sobre los objetos básicos:

* ReplicaSet
* Deployment
* StatefulSet
* DaemonSet
* Job

[[Minikube]]
## Minikube

* Minikube es una implementación ligera de Kubernetes que crea una máquina virtual localmente y despliega un cluster sencillo formado por un solo nodo.

* Minikube es una gran herramienta para el desarrollo de aplicaciones Kubernetes y permite características habituales como _LoadBalancer_, _NodePort_, volúmenes persistentes, _Ingress_, dashboard, reglas de acceso, y demás.

En la https://github.com/kubernetes/minikube[página de GitHub de Minikube] se encuentra información sobre el proyecto, https://kubernetes.io/docs/tasks/tools/install-minikube/[instalación] y otros temas de interés.

Una vez instalado, probaremos los comandos básicos:

* Iniciar un cluster: `minikube start` (La primera vez que ejecutemos este comando descargará la ISO de Minikube, que son unos 130 MB, y creará la máquina virtual correspondiente)

* Acceso al Dashboard de Kubernetes: `minikube dashboard`

* Una vez iniciado, se podrá interactuar con el cluster usando `kubectl` (que veremos en la sección <<kubectl el CLI para Kubernetes>>) como con cualquier cluster Kubernetes:

    - Iniciar un servidor: `kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080`

    - Exponer un servicio como un _NodePort_: `kubectl expose deployment hello-minikube --type=NodePort`
    
    - Abrir el endpoint del servicio en el navegador: `minikube service hello-minikube`

+    
El servidor de ejemplo iniciado muestra información sobre el cliente en el que se está ejecutando y sobre las cabeceras. Dicho servidor es expuesto en el cluster de Kubernetes como un _NodePort_. El resultado tras mostrarlo con `minikube service hello-minikube` será algo similar al de la figura siguiente.

+
image::SampleKubernetesService.png[]

+
Si ahora abrimos el dashboard, se mostraría algo similar a lo de la figura siguiente. En la figura se observa cómo ha sido creado el Deployment `hello-minikube`.

+
image::KubernetesDashboard.png[]
    
* Iniciar un segundo cluster local: `minikube start -p cluster2`

* Detener el cluster local: `minikube stop`

* Eliminar el cluster local: `minikube delete`

## `kubectl` el CLI para Kubernetes

Para la interacción con un cluster local o remoto de Kubernetes mediante comandos se usa `kubectl`, un CLI sencillo que nos permitirá realizar tareas habituales como despliegues, escalar el cluster u obtener información sobre los servicios en ejecución. `kubectl` es el CLI para interactuar con el servidor de la API de Kubernetes.

Consultar la https://kubernetes.io/es/docs/tasks/tools/install-kubectl/#instalar-kubectl[página oficial de instalación y configuración de `kubectl`]

Para interactuar con unos ejemplos sencillo con `kubectl` podemos

* Obtener información de la versión

* Obtener información del cluster

+
[source, bash]
----
$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
KubeDNS is running at https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
----

* Obtener los nodos que forman el cluster

+
[source, bash]
----
$ kubectl get nodes
NAME       STATUS   ROLES    AGE     VERSION
minikube   Ready    master   3d23h   v1.15.0
----

* Otras operaciones de interés son: 
    - `kubectl describe <resource>` para obtener información detallada sobre un recurso.
    - `kubectl logs <pod>` para mostrar los logs de un contenedor en un pod.
    - `kubectl exec <pod> <command>` para ejecutar un comando en un contenedor de un pod.

## Componentes de Kubernetes en acción

### Deployments

Una configuración de Deployment pide a Kubernetes que cree y actualice las instancias de una aplicación. Tras crear el Deployment, el Master organiza las instancias de aplicación en los nodos disponibles del cluster.

image::KubernetesDeployment.svg[]

Una vez creadas las instancias de aplicación, el *Controlador de Deployment de Kubernetes* monitoriza continuamente las instancias. Si un nodo en el que está una instancia cae o es eliminado, el Controlador de Deployment de Kubernetes sustituye la instancia por otra instancia en otro nodo disponible del cluster.

Esta funcionalidad de _autocuración_ de las aplicaciones supone un cambio radical en la gestión de las aplicaciones. Esta característica de recuperación de fallos mediante la creación de nuevas instancias que reemplazan a las defectuosas o desaparecidas no existía antes de los orquestadores.

Al crear un Deployment se especifica la imagen del contenedor que usará la aplicación y el número de réplicas que se quieren mantener en ejecución. El número de réplicas se puede modificar en cualquier momento actualizando el Deployment.

#### Despliegue de una aplicación

Podemos ejecutar una aplicación con `kubectl run` indicando el nombre que se dará al Deployment y el nombre de la imagen (Docker) usada para la aplicación.

[source, bash]
----
$ kubectl run jsonproducer --image=ualmtorres/jsonproducer:v0 --port 80 <1>

deployment.apps/jsonproducer created
----
<1> El puerto hace referencia al puerto que usa la aplicación original para servir su contenido.

Esto ha hecho que el Master haya buscado un nodo para ejecutar la aplicación, haya programado la ejecución de la aplicación en ese nodo y haya configurado el cluster para programar la ejecución de otra instancia cuando sea necesario.

[NOTE]
====
Para imágenes que no estén en Docker Hub se pasa la URL completa del repositorio de imágenes.
====

Para obtener los Deployments disponibles

[source, bash]
----
$ kubectl get deployments

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           8s
----

Para poder acceder a la aplicación deberemos primero exponerla en el cluster de Kubernetes. Más adelante veremos los detalles. Por ahora, basta con ejecutar el comando siguiente, el cual creará un _servicio_ asociado a nuestro Deployment para poder acceder a la aplicación. 

[source, bash]
----
$ kubectl expose deployment jsonproducer --type=NodePort

service/jsonproducer exposed
----

Para ver la ejecución de la aplicación, pediremos a Minikube que nos muestre el _servicio_ con el comando

[source, bash]
----
$ minikube service jsonproducer
----

Esto abrirá un navegador y el resultado del servicio es un JSON similar a este:

[source, json]
----
{"nombre":"manolo"}
----


### Pods

Al crear el Deployment anterior, Kubernetes creó un Pod para ejecutar una instancia de la aplicación. Un Pod es una abstracción de Kubernetes que representa un grupo de uno o más contenedores de una aplicación y algunos recursos compartidos de esos contenedores (p.e. volúmenes, redes)

[NOTE]
====
Un ejemplo de pod con más de un contenedor lo encontramos en lo que se denominan _sidecars_. Ejemplos de sidecar los encontramos en aplicaciones que registran su actividad en un contenedor (sidecar) dentro del mismo pod y publican la actividad en una aplicación que monitoriza el cluster. Otro ejemplo de sidecar es el de un contenedor sidecar que proporciona un certificado SSL para comunicación https al contenedor de la aplicación.
====

Los contenedores de un pod comparten una IP y un espacio de puertos, y siempre van juntos y se despliegan juntos en un nodo. La figura siguiente ilustra varias configuraciones de pods: Un pod con un contenedor, un pod con un contenedor y un volumen, un pod con dos contenedores que comparten un volumen y un pod con varios contenedores y varios volúmenes.

image::KubernetesPod.svg[]

Los pods son la unidad atómica de Kubernetes. Al crear un despliegue en Kubernetes, el Deployment crea Pods con contenedores en su interior. Cada pod queda ligado a un nodo y sigue allí hasta que se finalice o se elimine. En caso de fallo del nodo se planifica la creación de sus pods en otros nodos disponibles del cluster. *Los pods son efímeros, por lo que su almacenamiento desaparece al eliminar el pod*. Por este motivo es necesario saber utilizar almacenamiento externo para que los datos persistan. El almacenamiento se tratará en otra sección de este tutorial.


.Creación de un pod para MongoDB

Los pods, al igual que otros recursos de Kubernetes (replicasets, volúmenes, ...) se pueden crear sobre la marcha con el CLI indicando la imagen a partir de la que se crean, o se pueden crear a partir de archivos de manifiesto. Estos archivos de manifiesto se escriben en sintaxis https://yaml.org/[YAML] y representan una forma declarativa de definir los recursos del cluster Kubernetes. 

Para ilustrar cómo crear un pod, veremos cómo crear uno sencillo para MongoDB a partir de un archivo de manifiesto. Para ir familiriarizándonos con Kubernetes, probaremos también con unos comandos básicos para mostrar información, mostrar los logs, redirección de puertos

. Creación del manifiesto YAML 
+
Archivo `mongodb-basico.yaml`
+
[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
----

+
. Despliegue del manifiesto para crear el pod
+
[source, bash]
----
$ kubectl apply -f mongodb-basico.yaml
----

+
. Inicio de sesión SSH en el pod
+
[source, bash]
----
$ kubectl exec -it mongodb /bin/bash
----

+
. Mostrar información del pod
+
[source, bash]
----
$ kubectl describe pod mongodb
----

+
. Mostrar los logs del pod
+
[source, bash]
----
$ kubectl logs mongodb
----

+
. Redirección del puerto del pod a un puerto local (establece un túnel SSH entre nuestro equipo y el pod con los puertos indicados)
+
[source, bash]
----
$ kubectl port-forward mongodb 27017:27017
----

+
. Eliminación del pod
+
[source, bash]
----
$ kubectl delete -f mongodb-basico.yaml
----

### Nodos

Los pods se ejecutan en un Nodo. Un nodo es una máquina _worker_ (física o virtual) del cluster. Los nodos están gestionados por el Master. Un Nodo puede contener muchos pods.

image::KubernetesNode.svg[]

Cada Nodo ejecuta al menos:

* Kubelet, un proceso que se encarga de la comunicación entre el nodo y el Master. Gestiona los pods y los contenedores que se están ejecutando en el nodo.
* Un motor de contenedores, como Docker, que se encarga de la descarga de imágenes de un registro y de ejecutar la aplicación.

### Servicios

Se dice que en Kubernetes los pods son mortales o efímeros. Cuando un nodo desaparece (bien por un error o por una desconexión), los contenedores que están en el nodo también se pierden. A continuación, un _ReplicaSet_ se encarga de devolver al cluster al estado deseado y organiza la creación de nuevos pods en otros nodos disponibles para mantener funcionando la aplicación. Las réplicas de los pods han de ser intercambiables y *aunque cada pod en el cluster tenga su propia IP única, Kubernetes reconcialiará los cambios entre los pods para que las aplicaciones sigan funcionando*.

Los servicios en Kubernetes son una abstracción que definen un conjunto lógico de pods y una política de acceso a ellos estableciendo un nombre para acceder a ellos. Esto permite que haya un acoplamiento débil entre pods dependientes. El acceso puede ser interno o externo al cluster. De esta forma, las aplicaciones sólo usarán los nombres de los servicios y no las IP de los pods, ya que éstas nunca son fijas debido a que, por un lado, los pods se crean y se destruyen para mantener el número de réplicas deseado; y por otro lado, un pod puede ser sustituido por otro ante un problema y el nuevo pod tendrá una IP diferente.

Cada pod tiene una dirección IP única, pero esa IP no se expone fuera del cluster sin lo que se denomina un Servicio. Los servicios pemiten que las aplicaciones reciban tráfico. En función del ámbito de la exposición del servicio tenemos:

* ClusterIP: El servicio recibe una IP interna a nivel de cluster y hace que el servicio sólo sea accesible a nivel de cluster.
* NodePort: Expone el servicio fuera del cluster concatenando la IP del nodo en el que está el pod y un número de puerto entre 30000 y 32767, que es el mismo en todos los nodos
* LoadBalancer: Crea en cloud, si es posible, un balanceador externo con una IP externa asignada.
* ExternalName: Expone el servicio usando un nombre arbitrario (especificado en `externalName`)

image::KubernetesService.svg[]

Los servicios enrutan el tráfico entre los pods proporcionando una abstracción que permite que los pod mueran y se repliquen sin impactar en la aplicación. El descubrimiento y enrutado entre pods dependientes (p.e. frontend y backend) son gestionados por los Servicios.

Los servicios agrupan a sus pods usando etiquetas y selectores. Las etiquetas son pares clave-valor y tienen usos muy variados:

* Diferenciar entre objetos de desarrollo, prueba y producción
* Distinguir entre versiones

image::KubernetesLabels.svg[]

En la figura se observa cómo el selector de etiquetas usado en los Deployment sirve para agrupar los pods que conforman un servicio, ya que cada pod contiene la misma etiqueta usada en el selector del Deployment al que pertenece.

Las etiquetas se pueden configurar durante la creación o en cualquier momento posterior.

#### Ejemplo. Creación de un servicio

Anteriormente, en la sección <<Despliegue de una aplicación>> creamos una aplicación de ejemplo que generaba un JSON de prueba. A modo de recordatorio, hicimos lo siguiente:

1. Crear un Deployment a partir de la imagen `ualmtorres/jsonproducer:v0` de Docker Hub con el comando 

+
[source, bash]
----
$ kubectl run jsonproducer --image=ualmtorres/jsonproducer:v0 --port 80
----

+
Podemos consultar el Deployment existente con el comando siguiente. Si por cualquier motivo no se dispone del Deployment, basta con ejecutar el comando anterior para crearlo.

+
[source, json]
----
$ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           17m
----

+
Este Deployment habrá creado un pod que estará ejecutando la aplicación disponible de la imagen utilizada. Podemos ver los pods disponibles con el comando 

+
[source, bash]
----
$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          23m
----

2. Crear un servicio para poder exponer la aplicación al exterior. Concretamente usamos un servicio de tipo NodePort, lo que nos sirve la aplicación concatenando la IP del nodo donde está el pod y un puerto aleatorio. El servicio lo creamos con  

+ 
[source, bash]
----
$ kubectl expose deployment jsonproducer --type=NodePort
----

+
Podemos consultar el servicio existente con el comando siguiente. Si por cualquier motivo no se dispone del servicio, basta con ejecutar el comando anterior para crearlo.

+
[source, bash]
----
$ kubectl get services
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
jsonproducer   NodePort    10.99.116.165   <none>        80:30737/TCP   25m <1>
kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP        34d <2>
----
<1> Este es nuestro servicio. En el caso del tutorial, el puerto aleatorio asignado es el 30737
<2> Servicio `kubernetes` creado de forma predetermianda al iniciarse Minikube

+
Podemos acceder el servicio creado con

+
[source, bash]
----
$ minikube service jsonproducer
----

+
image::KubernetesRunningService.png[]

+
Si queremos consultar la información del servicio creado usaremos la opción `describe` de `kubectl` 

+
[source, bash]
----
$ kubectl describe services jsonproducer <1>

Name:                     jsonproducer
Namespace:                default
Labels:                   run=jsonproducer <2>
Annotations:              <none>
Selector:                 run=jsonproducer
Type:                     NodePort
IP:                       10.99.116.165
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  30737/TCP
Endpoints:                172.17.0.5:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
----
<1> Pasamos el nombre de nuestro servicio como parámetro
<2> Etiqueta añadida de forma predeterminada

+
Si ahora consultamos la información del pod de la aplicación veremos que coincide la etiqueta. Recordemos que al introducir el concepto de Servicio se indicó que era una abstracción para agrupar pods y que utilizaba etiquetas para poder reunirlos. He aquí la correspondencia entre la etiqueta del servicio y la etiqueta de los pods del servicio.

[source, bash]
----
$ kubectl get pods <1> 

NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          49m

$ kubectl describe pods jsonproducer-7769d76894-2nzt2 <2>

Name:               jsonproducer-7769d76894-2nzt2
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               minikube/10.0.2.15
Start Time:         Mon, 15 Jul 2019 18:56:20 +0200
Labels:             pod-template-hash=7769d76894
                    run=jsonproducer <3>
Annotations:        <none>
Status:             Running
IP:                 172.17.0.5
Controlled By:      ReplicaSet/jsonproducer-7769d76894 <4>
Containers:
  jsonproducer:
    Container ID:   docker://52e290262984a94da4dd89102b93d80f59c0c4310c303dac67b02884d73fb545
    Image:          ualmtorres/jsonproducer:v0 <5>
...
----
<1> Obtener primero los pods disponibles para poder acceder al pod deseado
<2> Obtener información del pod
<3> Etiqueta coincidente con el selector (etiqueta) del Deployment
<4> ReplicaSet encargado de mantener el número de pods deseados para el Deployment
<5> Imagen base usada para crear el único contenedor de este pod

### Volúmenes

Básicamente, uno volumen es un directorio para datos que es accesible a los contenedores de un Pod y que persiste a los reinicios de un Pod. El medio que se use para el almacenamiento y cómo se comporte ante una eliminación del Pod depende del tipo de volumen que se use.

Para usar un volumen, un Pod especifica el volumen que proporciona al Pod (el campo `.spec.volumes`) y donde montarlo en los contenedores (el campo `.spec.containers.volumeMounts`).

### Namespaces

Abstracción de Kubernetes para soportar varios clusters virtuales en un mismo cluster físico. Los namspaces se usan para organizar objetos en un cluster y para proporcionar una forma de dividir los recursos del cluster. Los nombres de los recursos tienen que ser únicos a nivel de namespace, pero no a nivel de cluster.

[TIP]
====
En clusters con varios usuarios los namespaces proporcionan una forma de agrupar los recursos de cada usuario. Además, los administradores pueden establecer cuotas a nivel de namespace limitando a los usuarios la cantidad de objetos que pueden crear y la cantidad de recursos del cluster que pueden consumir (p.e. CPU, memoria).
====

## Escalado de una aplicación

Hasta ahora hemos creado un Deployment que posteriomente ha sido expuesto mediante un Servicio. Como no indicamos número de réplicas, el Deployment creó sólo un Pod para ejecutar la aplicación. Si la demanda aumenta quizá puede llegar a ser necesario aumentar el número de pods de la aplicación. Esto es lo que se conoce como escalado y hace referencia al número de réplicas en un Deployment.

[NOTE]
====
Para escalar un Deployment durante la creación se usa el parámetro `--replicas=<numero-de-replicas>`.
====

Al escalar una aplicación se crearán nuevos pods en los nodos con recursos disponibles e irá aumentando hasta llegar al número de pods deseados. La ejecución de varias instancias trae consigo la distribución del tráfico entre todos los pods del Deployment. De esta tarea se encarga un balanceador de carga que integra el propio Servicio.

[NOTE]
====
Escalar a 0 terminará todos los pods de un Deployment.
====

Una vez que entramos en la dinámica de tener varias instancias de la misma aplicación, se pueden tener actualizaciones en caliente (_rolling updates_) sin suspensión del servicio. Esto lo veremos en la sección <<Actualización de aplicaciones>>.

### Ejemplo de escalado de una aplicación

En primer lugar veremos cuáles eran las condiciones del despliegue de ejemplo que estamos usando.

[source, bash]
----
$ kubectl get deployments

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           68m
----

* `READY` indica el ratio entre los pods deseados y los que están en ejecución.
* `UP-TO-DATE` indica el número de réplicas que están actualizadas para alcanzar el estado deseado.
* `AVAILABLE` indica el número de réplicas disponibles actualmente para los usuarios.

El comando siguiente escala a 4 réplicas el despliegue de ejemplo (`jsonproducer`)

[source, bash]
----
$ kubectl scale deployments jsonproducer --replicas=4

deployment.extensions/jsonproducer scaled
----

Unos instantes después podremos comprobar que el Deployment ya ha alcanzado el estado deseado.

[source, bash]
----
$ kubectl get deployments

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   4/4     4            4           73m
----

La aplicación sigue disponible sin ningún cambio para el usuario final. Sin embargo, ahora hay 4 réplicas cuyo tráfico es gestionado por un balanceador de carga asociado al servicio.

image::KubernetesRunningService.png[]

La información de las réplicas la podemos obtener consultando el número de pods con el comando siguiente:

[source, bash]
----
$ kubectl get pods

NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          74m
jsonproducer-7769d76894-9xdqw   1/1     Running   0          38s
jsonproducer-7769d76894-nhtl4   1/1     Running   0          38s
jsonproducer-7769d76894-qbvzd   1/1     Running   0          38s
----

Si ahora por cualquier motivo dejase de estar disponible alguno de los nodos en los que se encuentra desplegados los pods de la apliación, o bien dejase de funcionar alguno de los pods, el Controlador de Deployment de Kubernetes se encargaría de organizar la creación de nuevos pods para volver a alcanzar el estado deseado, en nuestro caso 4 réplicas.

Probemos esta funcionalidad eliminando el último pod y comprobando como Kubernetes organiza inmediatamente la creación de otro pod que lo sustituya.

[source, bash]
----
$ kubectl delete pods jsonproducer-7769d76894-qbvzd
pod "jsonproducer-7769d76894-qbvzd" deleted

$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          85m
jsonproducer-7769d76894-9xdqw   1/1     Running   0          12m
jsonproducer-7769d76894-gh7qk   1/1     Running   0          3s <1>
jsonproducer-7769d76894-nhtl4   1/1     Running   0          12m
----
<1> Pod que sustituye al pod eliminado creado automáticamente para mantener el número de réplicas a 4

Por último, si ahora queremos reducir el número de réplicas a 2 bastará con volver a indicarlo al Deployment en el parámetro `replicas` y este será el nuevo estado a alcanzar.

[source, bash]
----
$ kubectl scale deployments jsonproducer --replicas=2
deployment.extensions/jsonproducer scaled

$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          92m
jsonproducer-7769d76894-9xdqw   1/1     Running   0          18m
----

## Actualización de aplicaciones

Para poder realizar actualizaciones sin tener que suspender el servicio mientras se realiza la actualización, Kubernetes proporciona las _rolling updates_, que van actualizando los pods con la nueva versión de la aplicación.

De forma predeterminada, el número de pods que pueden estar no disponibles durante una actualización es 1, aunque esta opción es configurable, ya sea mediante cantidad o porcentaje de pods no disponibles durante la actualización. Además, es posible volver a una versión anterior.

Al igual que ocurre al escalar las aplicaciones, si el Despliegue está expuesto, el Servicio balancerá el tráfico sólo a los pods que estén disponibles durante la actualización.

A continuación se muestra cómo actualizar el Deployment de ejemplo `jsonproducer` con nuevo Deployment con el mismo nombre y una versión de la imagen. 

[source, bash]
----
$ kubectl set image deployments jsonproducer jsonproducer=ualmtorres/jsonproducer:v1
----

Al realizar la actualización de la imagen del Deployment, Kubernetes tendrá que descargar la nueva imagen y organizar la creación de los pods en los nodos con recursos disponibles. Mientras se realiza la actualización podremos ver que hay nodos que se están terminando, otros que se están creando y otros que están disponibles.

[source, bash]
----
$ kubectl get pods
NAME                            READY   STATUS              RESTARTS   AGE
jsonproducer-7769d76894-fr7cz   1/1     Running             0          25s
jsonproducer-7769d76894-hfpr7   1/1     Terminating         0          24s
jsonproducer-c76c87f-jwhxq      0/1     ContainerCreating   0          0s
jsonproducer-c76c87f-tmbkk      1/1     Running             0          1s
----

Tras unos instantes, la aplicación dejará de servir la versión anterior de la aplicación y comenzará a servir la nueva versión. La nueva versión de la aplicación sirve `Manolo Torres` en lugar de `manolo` en el JSON.

image::KubernetesUpdateImage.png[]

Para deshacer una actualización de una aplicación volviendo a la versión anterior haremos un `rollout undo`. El comando siguiente devuelve a la aplicación a la versión anterior

[source, bash]
----
$ kubectl rollout undo deployments jsonproducer
deployment.extensions/jsonproducer rolled back
----

Tras este comando, el Controlador de Deployment de Kubernetes irá reemplanzando los pods hasta alcanzar el estado deseado. A continuación se ve el estado intermedio mientras se vuelve a la versión anterior.

[source, bash]
----
$ kubectl get pods 
NAME                            READY   STATUS        RESTARTS   AGE
jsonproducer-7769d76894-m22sv   1/1     Running       0          2s
jsonproducer-7769d76894-v6hfv   1/1     Running       0          4s
jsonproducer-c76c87f-jwhxq      0/1     Terminating   0          14m
jsonproducer-c76c87f-tmbkk      0/1     Terminating   0          14m
----

Tras unos instantes, se alcanzará el estado deseado

[source, bash]
----
Caligari:~ manolo$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-m22sv   1/1     Running   0          8s
jsonproducer-7769d76894-v6hfv   1/1     Running   0          10s
----

Y la aplicación volverá a mostrar el contenido anterior.

image::KubernetesRunningService.png[]

## Despliegue de aplicaciones mediante archivos YAML

Hasta ahora, las interacción con Kubernetes la hemos hecho sobre la marcha, creando despliegues, servicios, escalado de aplicaciones y demás. Sin embargo, esta no es la forma habitual. Esta forma de uso de Kubernetes está más orientada a la creación de tareas puntuales. En cambio, cuando se trata de operaciones que queremos que sean repetibles, se suelen crear archivos YAML especificando el objeto que se quiere crear en Kubernetes (espacio de nombres, despliegue, servicio, ...). Una vez creados estos archivos, se usará `kubectl` para cargarlos/desplegarlos en Kubernetes.

[NOTE]
====
El uso de archivos para despliegues Kubernetes nos permitirá admeás someter nuestros archivos de despligue de recursos Kubernetes a control de versiones y poder distribuirlos fácilmente.
====

Para ilustrar el despliegue de una aplicación mediante archivos YAML vamos a desplegar una aplicación de ejemplo que consuma del servicio `jsonproducer` creado anteriormente. Se trata de un ejemplo muy sencillo de un entorno frontend-backend con un funcionamiento independiente. Esto, además de desacoplar la presentación del backend, desde el punto de vista de la escalabilidad, permite escaladar backend y frontend de forma independiente.

### Creación del archivo de Deployment 

Un archivo de Deployment proporciona una forma declarativa de creación de Pods y ReplicaSets. En el archivo de Deployment se especifica el estado deseado.

Vamos a crear un archivo de Deployment denominado `json-reader-deployment.yaml`. Este archivo básicamente contiene entre otros el nombre de despliegue, la etiqueta usada para agrupar los pods del servicio, número de réplicas y la imagen usada para crear el contenedor de cada pod.

[source, yaml]
----
apiVersion: apps/v1
kind: Deployment <1>
metadata:
  name: jsonreader <2>
  namespace: default <3>
  labels:
    app: jsonreader <4>
spec:
  revisionHistoryLimit: 2 <5>
  strategy:
    type: RollingUpdate <6>
  replicas: 2 <7>
  selector:
    matchLabels:
      app: jsonreader <8>
  template:
    metadata:
      labels:
        app: jsonreader
    spec:
      containers:
      - name: jsonreader <9>
        image: ualmtorres/jsonreader:latest <10>
        ports:
        - name: http
          containerPort: 80 <11>
----
<1> Tipo de recurso a desplegar
<2> Nombre del despliegue
<3> Namespace de despliegue
<4> Selector usado para agrupar a los pods del servicio asociado
<5> Número de versiones almacenadas para poder deshacer despliegues fallidos
<6> Tipo de estrategia de actualización
<7> Número de réplicas del despliegue
<8> Selector que define cómo el Deployment encuentra los Pods a gestionar, *que coincide con el definido en la plantilla (template) del pod*
<9> Prefijo usado para los pods
<10> Imagen base para los contenedores de la aplicación
<11> Puerto por el que la aplicación sirve originalmente sus datos

[NOTE]
====
La estrategia de despliegue (`spec.strategy.type`) puede ser `Recreate` o `RollingUpdate`, que es el valor predeterminado.
====

El despliegue se realiza con `kubectl` con el comando siguiente

[source, bash]
----
$ kubectl apply -f json-reader-deployment.yaml
----

Al crear el despliegue, se procederá a descargar la imagen y se pasarán a crear los dos pods indicados para este despliegue. Podemos ver los pods creados con el comando siguiente comprobando que efectivamente se creado los dos pods `jsonreader` que exigía el despliegue.

Podemos ver el despliegue con el comando siguiente

[source, bash]
----
$ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           22h
jsonreader     2/2     2            2           21h
----

También podemos ver los ReplicaSets creados por los despliegues

[source, bash]
----
$ kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
jsonproducer-7769d76894   1         1         1       22h
jsonreader-86699d9f94     2         2         2       22h
----

Los pods los podemos ver junto con sus etiquetas con el parámetro `--show-labels`

[source, bash]
----
$ kubectl get pods --show-labels
NAME                            READY   STATUS    RESTARTS   AGE   LABELS
jsonproducer-7769d76894-ss5qh   1/1     Running   1          22h   pod-template-hash=7769d76894,run=jsonproducer
jsonreader-86699d9f94-khfzh     1/1     Running   1          22h   app=jsonreader,pod-template-hash=86699d9f94
jsonreader-86699d9f94-lrvpt     1/1     Running   1          22h   app=jsonreader,pod-template-hash=86699d9f94
----

### Creación del archivo de Servicio

Un Servicio es una abstracción que define una agrupación de Pods y una política de acceso a ellos. El conjunto de Pods al que se dirige un Servicio están determinados por un *selector*.

Vamos a crear un archivo de Servicio denominado `json-reader-service.yaml`. Este archivo básicamente contiene entre otros el nombre de servicio, el tipo del servicio (ClusterIP, NodePort, ...), el puerto de acceso a los pods del desplieguw y el selector que identifica al despliegue con el que se corresponde el servicio creado.

[source, yaml]
----
apiVersion: v1
kind: Service <1>
metadata:
  name: jsonreader <2>
  namespace: default <3>
spec:
  type: NodePort <4>
  ports:
  - name: http
    port: 80 <5>
    targetPort: http
  selector:
    app: jsonreader <6>
----
<1> Tipo de recurso a desplegar
<2> Nombre del servicio
<3> Namespace de despliegue
<4> Tipo de servivio. NodePort hará que el servicio esté disponible en la IP de los nodos en los que estén los pods y un puerto aleatorio entre 30000 y 32767
<5> Puerto en el que los pods están sirviendo su contenido
<6> Etiqueta que tiene que coincidir con la usada en el Deployment

El despliegue se realiza con `kubectl` con el comando siguiente

[source, bash]
----
$ kubectl create -f json-reader-service.yaml
----

El despliegue nos permitirá acceder a la aplicación en un puerto en el rango 30000-32767. En este caso ha tocado el 31976

[source, bash]
----
$ kubectl get services
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
jsonproducer   NodePort    10.105.30.95   <none>        80:30228/TCP   22h
jsonreader     NodePort    10.99.85.2     <none>        80:31976/TCP   22h
kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP        22h
----


Para poder acceder al servicio pediremos a Minikube que nos lo muestre.

[source, bash]
----
$ minikube service jsonreader
----

Esto hará que se abra un navegador con la aplicación `jsonreader` que simplemente lee el JSON y presenta un saludo sencillo.

image::KubernetesServiceReader.png[]

También podemos usar el Kubernetes Dashboard para mostrar información de interés sobre este despliegue, viendo como de Deployment de `jsonreader` se ha incorporado a la lista de despliegues disponibles en el cluster, así como los pods, ReplicaSets y servicios, como muestran las figuras siguientes.

image::KubernetesDashboardJSON1.png[]

image::KubernetesDashboardJSON2.png[]

## Apéndice. Cheat Sheet

### Comandos Minikube

* `minikube version`
* `minikube start`
* `minikube dashboard`
* `minikube service <nombre-servicio>`
* `minikube delete`


### Comandos `kubectl`

* `kubectl version`
* `kubectl cluster-info`
* `kubectl get nodes|deployments|services|pods [--show-labels]` 
* `kubectl run <deployment> --image=<image> --port=<container-port>`
* `kubectl expose deployment <deployment>> --type=NodePort`
* `kubectl describe pods|deployments|services <resource>`
* `kubectl scale deployments <deployment> --replicas=<number-of-replicas>`
* `kubectl delete pods|deployments|services <resource>`
* `kubectl set image deployments <deployment> <deployment>=<image>`
* `kubectl rollout undo deployments <deployment>`
* `kubectl apply -f <filename-or-URL>`
* `kubectl logs <pod>`
* `kubectl exec <pod> <command>`

cloud_provider: 
  name: "openstack"
  openstackCloudProvider: 
    block_storage: 
      ignore-volume-az: true
      trust-device-path: false
    global: 
      auth-url: "http://192.168.64.12:5000/v3/"
      domain-name: "default"
      tenant-name: "mtorres"
      username: "mtorres"
      password: "xxx"
    load_balancer: 
      create-monitor: false
      manage-security-groups: false
      monitor-max-retries: 0
      use-octavia: false
    metadata: 
      request-timeout: 0

### Contextos

El archivo de contextos

Disponible en `~/.kube/config`

[source, bash]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/manolo/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: ""
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /Users/manolo/.minikube/client.crt
    client-key: /Users/manolo/.minikube/client.key
----

Obtener los contextos

[source, bash]
----
$ kubectl config get-contexts
CURRENT   NAME            CLUSTER         AUTHINFO     NAMESPACE
          minikube        minikube        minikube 
----

Añadir un contexto nuevo

Obtener los datos de conexión a Rancher desde 

image::RancherKubeconfig.png[]

Ahí aparecen los datos de conexión al cluster. Ahí se encuentran los datos que tenemos que copiar en el archivo `~/.kube/config`


image::KubeconfigCluster.png[]
image::KubeconfigUser.png[]

Editamos el archivo el archivo `~/.kube/config` y debería quedar algo así
[source, bash]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/manolo/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
- cluster: <1>
    certificate-authority-data: XXXXXXXXXXXXXXXXXXX
  name: produccion-ci
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
- context: <2>
    cluster: produccion-ci
    namespace: mtorres
    user: user-XXXXXX
  name: produccion-ci
current-context: ""
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /Users/manolo/.minikube/client.crt
    client-key: /Users/manolo/.minikube/client.key
- name: user-XXXXX <3>
  user:
    token: XXXXXXXXXXXXXXXXXXXXX
----
<1> Datos del cluster de Rancher
<2> Datos del nuevo contexto
<3> Datos del usuario

Usar un contexto

[source, bash]
----
$ kubectl config use-context produccion-ci

Switched to context "produccion-ci".
----

Si ahora consultamos los contextos, veremos que el contexto activo es `produccion-ci`. Por tanto, todas las operaciones que hagamos con `kubectl` a partir de ahora se dirigirán contra ese contexto (cluster-usuario-namespace).

[source, bash]
----
$ kubectl config get-contexts
CURRENT   NAME            CLUSTER         AUTHINFO     NAMESPACE
          minikube        minikube        minikube     
*         produccion-ci   produccion-ci   user-mzmh8   mtorres
----

## Apéndice. Service Mesh con Istio

Los microservicios se han convertido en algo habitual en las aplicaciones cloud actuales. Las arquitecturas de microservicios permiten realizar cambios en un servicio sin tener que volver a desplegar toda la aplicación. A diferencia de otras arquitecturas, los microservicios pueden ser creados por grupos de desarrollo pequeños creando sus propias herramientas y lenguajes de programación. Básicamente, los microservicios se construyen de forma independiente, se comunican entre sí y pueden fallar de forma individual sin provocar una caída del funcionamiento de la aplicación completa.

Sin embargo, el desarrollo y la utilización de microservicios supone nuevos desafíos e implica el desarrollo y gestión de comunicación entre servicios. Esta lógica podría ser codificada en cada servicio sin necesidad de un _service mesh_, pero a medida que la complejidad crece, se hace más necesario un _service mesh_.

Un _service mesh_ es una capa complementaria responsable de la gestión del tráfico, políticas, certificados y seguridad de los servicios. Así, un _service mesh_ no añade nueva funcionalidad a las aplicaciones. Simplemente, se dedica a sacar fuera de los servicios la lógica de comunicación y la abstrae a una capa de infraestructura. Con un _service mesh_ los desarrolladores pueden centrarse en el desarrollo de la lógica de negocio y abstraerse de lo demás.

Para ofrecer esta funcionalidad, un _service mesh_ introduce una colección de proxies de red. En un _service mesh_ las peticiones entre los servicios se enrutan a través de los proxies en su propio nivel de infraestructura. Estos proxies son implementados como sidecars que se situan en el mismo pod que el servicio al que _sirven_ (tráfico, seguridad, ...). Si vemos esta red de sidecars desacoplados de los servicios y los situamos en una capa aparte visualizaremos el _service mesh_.

image::ServiceMesh.png[]

La idea entonces es inyectar un contenedor sidecar especial en cada microservicio y enrutar todo el tráfico a través de los sidecars en lugar de a través de los microservicios. El controlador del _service mesh_ podrá filtrar tráfico, y aplicar políticas de balanceo, seguridad y limitación de tráfico.

Sin un _service mesh_, cada microservicio debería incluir la lógica de gobierno y comunicación con otros servicios, lo que añade una complejidad extra al desarrollo del servicio. Además, el disponer de un _service mesh_ en entornos normalizados, permite tratar el problema del tráfico, políticas, seguridad y certificados entre servicios de forma estándar, independientemente de la plataforma en la que estén desplegadas nuestras aplicaciones.

https://istio.io/[Istio] es un _service mesh_ que permite conectar, asegurar, controlar y observar servicios. 

https://www.kiali.io/[Kiali] extiende estas características de gestión del tráfico incorporando observabilidad y visualización de servicios de la red para mejorar el trabajo con la red. Kiali ofrece una forma sencilla de ver la toplogía de un _service mesh_ y observar cómo interactúan los servicios.


https://www.jaegertracing.io[Jaeger] se encarga del tracing y permite a los desarrolladores fragmentar una petición y ver cómo va avanzando entre los distintos servicios de princicio a fin.

image::ServiceMeshKialiJaeger.png[]

### Istio

Istio viene a incorporarse al vocabulario marinero y ballenero del ecosistema de Docker y Kubernetes. Istio es una palabra griega que significa _navegar_.

[NOTE]
====
Istio está disponible en Rancher desde la versión 2.3.0-alpha5. Basta activarlo en el menú `Tools`. Pedirá si se quiere realizar la inyección automática de sidecars en un _namespace_. Esto hará que se cree un sidecar en cada pod del _namespace_ para el `Istio-proxy`. *Este proxy intercepta todo el tráfico al microservicio del pod y asumirá la gestión del enrutado, la selección de versiones, el registro de actividad y tráfico, y el control de acceso*. Por tanto, en cada _namespace_ en el que quede activado Istio se tendrá configurada la etiqueta `istio-injection=enabled`. No obstante, también es posible activarlo de forma manual, lo que exigiría un reinicio de los servicios, despliegues y otros objetos Kubernetes para que se active el funcionamiento de Istio.

====

La figura siguiente ilustra una aplicación sin Istio. En ella cada microservicio es el responsable de implementar la funcionalidad de discovery, balanceo, resilencia, métricas y trazado.

image::beforeIstio.jpg[]

La figura siguiente ilustra como en as aplicaciones basadas en Istio los pods están formados por dos contenedores: el contenedor del microservicio y el sidecar. En el sidecar se delegan las tareas de discovery, balanceo, resilencia, métricas y trazado, lo que facilita el desarrollo de los microservicios.

image::afterIstio.jpg[]

Istio ofrece una forma declarativa, mediante la creación de manifiestos YAML, de gestión del tráfico, enrutado selectivo de peticiones (en lugar del round robin que ofrece Kubernetes), despliegues (_canary, A/B, blue/green_), resilencia a nivel de red (con opciones de _retry_, _timeout_), control de acceso, observación de microservicios distribuidos comprendiendo los flujos y trazas y pudiendo ver las métricas importantes de forma inmediata, inyección de caos para poner a prueba la resilencia de aplicaciones y servicios, por citar algunas de sus funcionalidades destacadas.

Para activar el uso de Istio en un namespace se haría con 

[source, bash]
----
`kubectl label namespace default istio-injection=enabled`
----


### Arquitectura de Istio

Instio consta de un plano de control y un plano de datos. El plano de datos está formado por proxies que están en la arqutiectura de la aplicación. Usando el patrón del sidecar, cada instancia de la aplicación tendrá su proxy dedicado a través del cual pasa todo el tráfico antes de llegar a la aplicación. Estos proxies individuales pueden ser configurados individualmente para enrutar, filtrar y aumentar el tráfico según sea necesario.

image::istioArchitecture.jpg[]

Además, Istio permite realizar deciciones de enrutado en función de las cabeceras HTTP (p.e. tipo de navegador, usuario, ...)

image::istioCanary.jpg[]

[NOTE]
====
Algo a tener en cuenta es que los componentes del plano de control son aplicaciones sin estado lo que permitan que puedan escalar horizontalmente. Todos los datos están almacenados en _etcd_ como descricpciones personalizadas de recursos Kubernetes.
====

Sin embrgo, toda esta funcionalidad tiene un coste sobre la infraestructura. Cuando mayor sea el cluster mayor será la carga añadida al sistema. Cada sidecar consume mucha RAM (unos 350Mb). Además, añade una latencia de unos 10 ms a cada petición.


### Control de tráfico y técnicas de despliegue 

* Despliegue _canary_: Se despliega en producción una nueva versión del código pero sólo se dirige a él una parte del tráfico. Quizá sólo tengan acceso a él clientes de prueba, empleados, usuarios de iOS, etcétera. Una vez desplegado el canario, se monitoriza para comprobar la existencia de excepciones, comportamiento no satisfactorio, bajada del rendimiento, y demás. En cambio, si el canario no muestra indicios de que presente problemas se pueden ir aumentando paulatinamente el tráfico hacia él. Si presenta un comportamiento inaceptable, se puede retirar fácilmente de producción.

* Control del tráfico: Se pueden especificar reglas de enrutado que controlen el tráfico a un conjunto de pods. En concreto, Istio usa los recursos `DestinationRule` y `VirtualService` en forma de manifiestos YAML para describir estas reglas.
    - `DestinationRule`: Define grupos (_subsets_) de pods.
    - `VirtualService` dirige el tráfico a un _subset_ basado en porcentajes, cabeceras, direcciones IP, por citar algunas. La selección de pods afectados es similar al modelo de selectores utilizado por Kubernetes para selección basada en etiquetas (_labels_).
    
+
Este comportamiento del enrutado no es sólo para el tráfico de entrada externo. Es para toda la comunicación inter-servicio en el _service mesh_. Así, si hubiese servicios desplegados en Kubernetes, pero que no sean parte del _mesh_ no estaría afectado por estas reglas y se regiría por las reglas de balanceo de Kubernetes (round-robin).

* _Dark launch_: Despliegue a producción que no es visible a los clientes. En este caso Istio permite duplicar (_mirror_) el tráfico a una versión de la aplicación y ver cómo se comporta respecto a la versión del pod en producción. De esta forma se están realizando peticiones en las condiciones de producción al nuevo servicio sin afectar al tráfico de la versión en producción. No obstante, hay que tener una consideración especial con los servicios que traten con datos o estén vinculados a otros servicios, para no introducir duplicados, provocar inconsistencias y otros problemas derivados de la duplicación de peticiones.

### Técnicas de resilencia

* _Circuit breaker_: Determina el número máximo de peticiones que puede soportar un pod. Pasado ese valor no admite más hasta que se recupere.
* _Pool ejection_: Saca de un nodo a un pod que esté dando fallos creando un nuevo pod que los sustituya en otro nodo.
* _Retries_: Reenvía la petición a otro pod al encontrar un caso de circuit breaker o pool rejection 

### Caso práctico

Para no perdernos en los detalles usaremos un ejemplo muy sencillo con dos servicios: uno que produce datos y otro que los presenta. Podríamos ver este ejemplo como un ejemplo muy reducido de backend y frontend.

El servicio que genera datos se denomina `jsonproducer` y genera un documento JSON con un único elemento `nombre` y un valor aasociado (p.e. `{"nombre": "manolo"}`). De este servicio se cuenta con dos versiones, cada una con su imagen Docker correspondiente. La primera versión devuelve el elemento JSON `{"nombre": "manolo"}`. La segunda versión devuelve el elemento JSON `{"nombre": "Manuel Torres"}`

El servicio que consume datos se denomina `jsonconsumer` y usará los datos leídos de `jsonproducer` para presentarlos al usuario en forma de saludo, mostrando `Hola` seguido del nombre leído del JSON devuelto por la versión de `jsonproducer` usada.

#### Creación de todos los servicios 

Vamos a desplegar en el cluster de Kubernetes todos los recursos (`Service` y `Deployment`) con todas sus versiones correspondientes. Posteriormente, con Istio controlaremos el tráfico que se dirige a cada versión desplegada. En nuestro ejemplo se definirán dos objetos `Deployment`, uno para cada una de las versiones del `jsonproducer` que quedarán desplegadas en el cluster.

Cada despliegue incorpora en los metadatos el nombre que le que queremos dar, así como unas etiquetas con su versión, que le permitirán ser seleccionado posteriormente cuando se definan los _servicios virtuales_. Además, en la `spec` del despliegue se usarán etiquetas en `matchLabels` que permitirán más adelante a Istio distinguir los pods correspondientes a cada despliegue.

En este ejemplo usaremos dos versiones (dos recursos `Deployment`) del `jsonproducer`. La primera está basada en la imagen `ualmtorres/jsonproducer:v0` que devuelve `{"nombre": "manolo"}`. La segunda está basada en la imagen `ualmtorres/jsonproducer:v1` que devuelve `{"nombre": "Manuel Torres"}`. Con este ejemplo tan sencillo nos bastará para ver a Istio en acción controlando el tráfico. 

[source, bash]
----
#########################################################
# jsonproducer service
#########################################################
apiVersion: v1
kind: Service
metadata:
  name: jsonproducer
  labels:
    app: jsonproducer
    service: jsonproducer
spec:
  ports:
  - port: 80
    name: http
  selector:
    app: jsonproducer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jsonproducer-v0 <1>
  labels:
    app: jsonproducer
    version: v0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jsonproducer
      version: v0 <2>
  template:
    metadata:
      labels:
        app: jsonproducer
        version: v0
    spec:
      containers:
      - name: jsonproducer
        image: ualmtorres/jsonproducer:v0 <3>
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jsonproducer-v1 <4>
  labels:
    app: jsonproducer
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jsonproducer
      version: v1 <5>
  template:
    metadata:
      labels:
        app: jsonproducer
        version: v1
    spec:
      containers:
      - name: jsonproducer
        image: ualmtorres/jsonproducer:v1 <6>
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
---
###########################################################
# jsonreader services
###########################################################
apiVersion: v1
kind: Service
metadata:
  name: jsonreaderpage
  labels:
    app: jsonreaderpage
    service: jsonreaderpage
spec:
  ports:
  - port: 80
    name: http
  selector:
    app: jsonreaderpage
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jsonreaderpage-v0
  labels:
    app: jsonreaderpage
    version: v0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jsonreaderpage
      version: v0
  template:
    metadata:
      labels:
        app: jsonreaderpage
        version: v0
    spec:
      containers:
      - name: jsonreaderpage
        image: ualmtorres/jsonreader:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
---
----
<1> Versión `v0` del servicio
<2> Selector para determinar los pods asociados a la versión `v0` del servicio
<3> Imagen `v0` del servicio
<4> Versión `v1` del servicio
<5> Selector para determinar los pods asociados a la versión `v1` del servicio
<6> Imagen `v1` del servicio

#### Creación de los _subsets_ mediante `DestionationRule`

Las `DestinatioRule` se usan para definir las distintas instancias o versiones disponibles de cada servicio. Cada servicio tendrá su `DestinationRule` con lo siguiente:

* `metatada.name`: Nombre.
* `spec.host`: Host contra el que se lanzará este servicio. Puede ser un nombre DNS (admite _wildcards_) o un nombre de servicio válido en nuestra aplicación.
* `spec.subsets`: Lista de versiones de servicios a configurar. Cada versión tendrá su nombre (`name`) y usará una etiqueta (p.e. `labels.version: v0`) para emparejarse con los pods de su versión de acuerdo a lo definido en el selector `matchLabels` del objeto `Deployment` del manifiesto del apartado anterior.

[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: jsonreaderpage
spec:
  host: jsonreaderpage
  subsets:
  - name: v0
    labels:
      version: v0
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: jsonproducer
spec:
  host: jsonproducer <1>
  subsets:
  - name: v0
    labels:
      version: v0
  - name: v1 <2>
    labels:
      version: v1 <3>
---
----
<1> Nombre que le damos a nuestro host y que luego será usado por los _servicios virtuales_ para dirigir el tráfico a una versión concreta de las definidas en los `subsets`. Este nombre debe coincidir con los nombres de servicio usados en el código de la aplicación
<2> Nombre dado a esta versión del servicio
<3> Etiqueta usada para seleccionar los pods a los que corresponde esta versión. Se emparejarán los pods que tengan `version: v1` en su `MatchingLabels`

#### Creación de los servicios virtuales para el control del tráfico

Por último, crearemos los `VirtualService` para indicarle a Istio la versión concreta de cada microservicio desplegado a la que queremos desviar el tráfico. Este recurso es el que Istio usará para configurar los proxies que controlarán el tráfico en el _mesh_.

Con los servicios virtuales conseguimos crear la capa complementaria a la aplicación que controlará su tráfico. Esto nos permite usar y cambiar a versiones concretas, derivar un porcentaje del tráfico a versiones concretas (despliegues `canary`), tener versiones diferentes para usuarios diferentes, control de tráfico basado en CIDR, y demás. 


[NOTE]
====
Con Istio podremos cambiar el enrutado a unos servicios u otros de forma dinánima. Basta con aplicar otro manifiesto con los nuevos valores de enrutado de los `VirtualService` que seleccionen las versiones correspondientes, el porcentaje de derivación de tráfico entre versiones que coexistan, y demás.
====

El manifiesto siguiente define un servicio virtual para cada servicio de nuestra aplicación. En este ejemplo cada servicio virtual usa la versión `v0` de los `Deployment` desplegados en el cluster.

[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonreaderpage
spec:
  hosts:
  - jsonreaderpage
  http:
  - route:
    - destination:
        host: jsonreaderpage
        subset: v0
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonproducer
spec:
  hosts:
  - jsonproducer <1> 
  http:
  - route:
    - destination:
        host: jsonproducer <2>
        subset: v0 <3>
---
----
<1> Nombre DNS (admite prefijos _wildcard_) o nombres de servicios del _mesh_
<2> Servicio al que se quiere dirigir el tráfico
<3> Versión a la que se quiere dirigir el tráfico

### Casos prácticos

. Instalación del ejemplo
+
[source, bash]
----
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.2/samples/bookinfo/platform/kube/bookinfo.yaml
$ kubectl get services
$ kubectl get pods
$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o "<title>.*</title>"
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.2/samples/bookinfo/networking/bookinfo-gateway.yaml
$ kubectl get gateway
----

. Crear el Ingress en Rancher para el servicio `productpage`. 
. Crear las reglas de acceso predeterminadas para los servicios de la aplicación

+
[source, bash]
----
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.2/samples/bookinfo/networking/destination-rule-all.yaml
$ kubectl get destinationrules -o yaml
----

Archivo `destination-rule-all.yaml` que permite el tráfico de forma indistinta a cualquier versión de `ratings`, `reviews` y `details`
[source, yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: productpage
spec:
  host: productpage
  subsets:
  - name: v1
    labels:
      version: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: reviews
spec:
  host: reviews
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
  - name: v3
    labels:
      version: v3
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: ratings
spec:
  host: ratings
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
  - name: v2-mysql
    labels:
      version: v2-mysql
  - name: v2-mysql-vm
    labels:
      version: v2-mysql-vm
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: details
spec:
  host: details
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
---
----

Usar sólo la versión V1 de cada microservicio:

[source, yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: productpage
spec:
  hosts:
  - productpage
  http:
  - route:
    - destination:
        host: productpage
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: ratings
spec:
  hosts:
  - ratings
  http:
  - route:
    - destination:
        host: ratings
        subset: v1
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: details
spec:
  hosts:
  - details
  http:
  - route:
    - destination:
        host: details
        subset: v1
---
----


Aplicando ahora unas reglas para desviar el tráfico a V2 a los usuarios con sesión iniciada con el usuario `jason` y a V1 a los que no tienen sesión iniciada.

[source, bash]
----
$ kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.2/samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
----

[source, yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
    - reviews
  http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  - route:
    - destination:
        host: reviews
        subset: v1
----
