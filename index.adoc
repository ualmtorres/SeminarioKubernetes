////
NO CAMBIAR!!
Codificación, idioma, tabla de contenidos, tipo de documento
////
:encoding: utf-8
:lang: es
:toc: right
:toc-title: Tabla de contenidos
:doctype: book
:imagesdir: ./images
:icons: font
:linkattrs:

////
Nombre y título del trabajo
////
# Kubernetes
Cloud-DI Team - Departamento de Informática. UAL

image::di.png[]

// NO CAMBIAR!! (Entrar en modo no numerado de apartados)
:numbered!: 

[abstract]
== Resumen
////
COLOCA A CONTINUACION EL RESUMEN
////

[IMPORTANT]
====
*TRABAJO EN CURSO*
====

////
COLOCA A CONTINUACION LOS OBJETIVOS
////
.Objetivos
* Conocer la importancia del uso de Kubernetes en el ciclo de vida de las aplicaciones actuales.
* Introducir los conceptos básicos de Kubernetes.
* Usar Minikube para el desarrollo y prueba en local.
* Usar `kubectl` para la administración básica de Kubernetes.
* Escalar aplicaciones.
* Realizar actualizaciones en caliente (_rolling updates_).
* Desplegar aplicaciones con archivos YAML.
* Aprender a usar Kubernetes Dashboard.
    
[TIP]
====
Disponibles los repositorios usados en este seminario:

* https://github.com/ualmtorres/json-producer[Código fuente de JSON Producer,window=_blank]
* https://github.com/ualmtorres/json-reader[Código fuente de JSON Reader,window=_blank]
* https://github.com/ualmtorres/jsonproducerreader[Archivos YAML de JSON Producer y Reader,window=_blank]
====
// Entrar en modo numerado de apartados
:numbered:

## Introducción

La adopción de Docker sigue creciendo de forma imparable y cada vez más organizaciones lo usan en producción. Por tanto, se hace necesario contar con una plataforma de orquestación que permita administrar y escalar los contenedores.

Supongamos que hemos comenzado a usar Docker y hemos hecho un despliegue de un par de servidores. De pronto, la aplicación comienza a tener un gran tráfico de entrada y hay que escalar a una gran cantidad de servidores para atender la demanda. Aquí es donde entra Kubernetes para hacer tareas del tipo _dónde debe ir un contenedor, cómo se monitorizan esos contenedores_ o _cómo se reinician cuando tengan un problema_.

## Conceptos básicos

Kubernetes es una plataforma de código abierto para despliegue automático, escalado y gestión de aplicaciones contenedorizadas. 

[quote,Documentación oficial de Kubernetes (https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/)]
____
Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.

The name Kubernetes originates from Greek, meaning helmsman or pilot. Google open-sourced the Kubernetes project in 2014. Kubernetes builds upon a decade and a half of experience that Google has with running production workloads at scale, combined with best-of-breed ideas and practices from the community.


____
Kubernetes ofrece una abstracción en la que permite el despliegue de aplicaciones en un cluster sin pensar en las máquinas que lo soportan. 

### Cluster de Kubernetes

Un cluster de Kubernetes está formado por dos tipos de recursos:

* El *Master* coordina el cluster. Coordina todas las actividades del cluster como organizar (schedule) las aplicaciones, mantener el estado deseado de las aplicaciones, escalado, despliegue de actualizaciones, y demás. También recoge información de los nodos worker y Pods.
* Los *Nodos* son _workers_ que ejecutan las aplicaciones. Cada nodo contiene un agente denominado _Kubelet_ que gestiona el nodo y mantiene la comunicación con el Máster. El nodo también tiene herramientas para trabajar con contenedores, como Docker.

[NOTE]
====
Un cluster Kubernetes en producción debería tener al menos 3 nodos. En entornos de producción se usan varios nodos máster para que los clusters sean tolerantes a fallos y ofrezcan alta disponibilidad.
====

image::KubernetesCluster.svg[]

Al desplegar una aplicación en Kubernetes el Master inicia los contenedores de la aplicación. El máster organiza los contenedores para que se ejecuten en los nodos del cluster. Los nodos se comunican con el master usando la https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.15/#-strong-api-overview-strong-[API de Kubernetes,window=_blank]. La API es expuesta a través del nodo Master y es posible usarla directamente para intectuar con el cluster.

.Lista de pods usando la API de Kubernetes
====
[source, bash]
----

$ curl http://<kubernetes_home>/api/v1/namespaces/default/pods
----

[source, json]
----
{
  "kind": "PodList",
  "apiVersion": "v1",
  "metadata": {
    "selfLink": "/api/v1/namespaces/default/pods",
    "resourceVersion": "10803"
  },
  "items": [
    {
      "metadata": {
        "name": "hello-minikube-64c7df9db-ffwtn",
        "generateName": "hello-minikube-64c7df9db-",
        "namespace": "default",
        "selfLink": "/api/v1/namespaces/default/pods/hello-minikube-64c7df9db-ffwtn",
        "uid": "652c298a-6dc2-4aec-a72f-390669fed6d2",
        "resourceVersion": "10608",
        "creationTimestamp": "2019-07-08T18:02:23Z",
        "labels": {
          "pod-template-hash": "64c7df9db",
          "run": "hello-minikube"
        },
....
----
====

Los clusters de Kubernetes se pueden desplegar sobre máquinas físicas o virtuales. Para comenzar a practicar con Kubernetes o para tareas de desarrollo, https://github.com/kubernetes/minikube[Minikube,window=_blank] es una buena opción. En la sección <<Minikube>> se presenta más información sobre esta plataforma. Minikube está disponible para Windows, Linux y MacOS.

### Arquitectura de Kubernetes

Tal y como hemos introducido en el apartado anterior, un cluster de Kubernetes está formado por dos tipos de unidades, el nodo _Master_ y los nodos _Worker_ (o siemplemente _Nodos_).

La figura siguiente ilustra estas dos unidades, así como algunos de los componentes más importantes en su interior.

image::KubernetesArchitecture.png[]

* Plugins de red: Permite la conexión entre pods de nodos diferentes y la integración de soluciones de red diferentes (overlay, L3, ...)
* `etcd`: una base de datos clave-valor donde Kubernetes guarda todos los datos del cluster.
* API server: Componente del Master que expone la API de Kubernetes. Es el front-end del plano de control de Kubernetes.
* Control Manager: Se encarga de comprobar si el estado deseado coincide con la realidad (p.e. número de réplicas)
* Scheduler: Componente del master que observa qué pods se han creado nuevos y no tienen nodo asignado, y les selecciona un nodo donde se puedan ejecutar.
* `kubelet`: Agente que se se ejecuta en cada nodo worker del cluster y que asegura que los nodos están en ejecución y sanos. *`kubelet` no gestiona los pods que no han sido creados por Kubernetes.* 
* `kube-proxy`: Mantiene las reglas de networking en los nodos para los pods que se ejecutan en él de acuerdo con las especificaciones de los manifiestos.
* `cAdvisor`: Recoge datos de uso de los contenedores.
* Plano de control o _Control plane_: Nivel de orquestación de contenedores que expone la API para definir, desplegar y gestionar el ciclo de vida de los contenedores.
* Plano de datos o _Data Plane_: Nivel que proporciona los recursos, como CPU. memoria, red y almacenamiento, para que los pods se puedan ejecutar y conectar a la red.

[TIP]
====
https://etcd.io/[etcd,window=_blank], es una base de datos clave-valor fiable y distribiuda para los datos más críticos de un un sistema distribuido. Dado que Kubernetes guarda todos los datos del cluster en ella, se deberían mantener copias de seguridad de esta base de datos y disponer de un plan de recuperación ante posibles desastres.
====

[NOTE]
====
Los componentes `kube-proxy`, `kube-scheduler`, `kube-controller-manager`, `etcd`, `kubelet`, así como los componentes de red se eejcutan como contenedores en cada uno de los nodos del cluster de Kubernetes. Basta con abrir un terminal en uno de los nodos del cluster y comprobarlo. Si lo hacemos, veremos como en los nodos worker están los contenedores de los componentes de Kubernetes junto con los contenedores de las aplicaciones que se están ejecutando en el nodo.

Un ejercicio interesante es detener el contenedor `kubelet` y ver cómo el nodo pasa a estar inactivo. En caso de ser el único nodo de trabajo, los contenedores de los nuevos despliegues quedarán en el estado `Pending` mientras `kubelet` no vuelva a estar disponible.
====
### Objetos de Kubernetes

Kubernetes ofrece una serie de objetos básicos y una serie de abstracciones de nivel superior llamadas Controladores. Los Controladores se basan en los objetos básicos y proporcionan funcionalidades adicionales sobre los objetos básicos

Los objetos básicos de Kubernetes son:

* Pod
* Service
* Volume
* Namespace

Los objetos de nivel superior o Controladores se basan en los objetos básicos y ofrecen funcionalidades adicionales sobre los objetos básicos:

* ReplicaSet
* Deployment
* StatefulSet
* DaemonSet
* Job

[[Minikube]]
## Minikube

* Minikube es una implementación ligera de Kubernetes que crea una máquina virtual localmente y despliega un cluster sencillo formado por un solo nodo.

* Minikube es una gran herramienta para el desarrollo de aplicaciones Kubernetes y permite características habituales como _LoadBalancer_, _NodePort_, volúmenes persistentes, _Ingress_, dashboard, reglas de acceso, y demás.

En la https://github.com/kubernetes/minikube[página de GitHub de Minikube,window=_blank] se encuentra información sobre el proyecto, https://kubernetes.io/docs/tasks/tools/install-minikube/[instalación,window=_blank] y otros temas de interés.

Una vez instalado, probaremos los comandos básicos:

* Iniciar un cluster: `minikube start` 

[NOTE]
====
La primera vez que ejecutemos este comando descargará la ISO de Minikube, que son unos 130 MB, y creará la máquina virtual correspondiente. Después, la preparará para Kubernetes y tras unos minutos estará disponible minikube en nuestro puesto de trabajo.
====

* Acceso al Dashboard de Kubernetes: `minikube dashboard`

* Una vez iniciado, se podrá interactuar con el cluster usando `kubectl` (que veremos en la sección <<kubectl el CLI para Kubernetes>>) como con cualquier cluster Kubernetes:

    - Iniciar un servidor: `kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080`

    - Exponer un servicio como un _NodePort_: `kubectl expose deployment hello-minikube --type=NodePort`
    
    - Abrir el endpoint del servicio en el navegador: `minikube service hello-minikube`

+    
El servidor de ejemplo iniciado muestra información sobre el cliente en el que se está ejecutando y sobre las cabeceras. Dicho servidor es expuesto en el cluster de Kubernetes como un _NodePort_. El resultado tras mostrarlo con `minikube service hello-minikube` será algo similar al de la figura siguiente.

+
image::SampleKubernetesService.png[]

+
Si ahora abrimos el dashboard, se mostraría algo similar a lo de la figura siguiente. En la figura se observa cómo ha sido creado el Deployment `hello-minikube`.

+
image::KubernetesDashboard.png[]

Si ahora probamos a eliminar el pod creado, veremos que se vuelve a crear. El objeto Deployment `hello-minikube` creado anteriormente se encarga de mantener el número de réplicas especificado (1 de forma predeterminada). Realmente tendremos que eliminar el objeto Deployment, y un poco más adelante veremos cómo hacerlo.
    
.Otros comandos interesantes de minikube
****
* Iniciar un segundo cluster local: `minikube start -p cluster2`

* Detener el cluster local: `minikube stop`

* Eliminar el cluster local: `minikube delete`
****


## `kubectl` el CLI para Kubernetes

Para la interacción con un cluster local o remoto de Kubernetes mediante comandos se usa `kubectl`, un CLI sencillo que nos permitirá realizar tareas habituales como despliegues, escalar el cluster u obtener información sobre los servicios en ejecución. `kubectl` es el CLI para interactuar con el servidor de la API de Kubernetes.

Consultar la https://kubernetes.io/es/docs/tasks/tools/install-kubectl/#instalar-kubectl[página oficial de instalación y configuración de `kubectl`,window=_blank]

Para interactuar con unos ejemplos sencillo con `kubectl` podemos

* Obtener información de la versión

* Obtener información del cluster

+
[source, bash]
----
$ kubectl cluster-info
Kubernetes master is running at https://192.168.99.100:8443
KubeDNS is running at https://192.168.99.100:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
----

* Obtener los nodos que forman el cluster

+
[source, bash]
----
$ kubectl get nodes
NAME       STATUS   ROLES    AGE     VERSION
minikube   Ready    master   3d23h   v1.15.0
----

* Otras operaciones de interés son: 
    - `kubectl get pods` para listar todos los pods desplegados.
    - `kubectl get all` para listar todos los objetos desplegados.
    - `kubectl describe <resource>` para obtener información detallada sobre un recurso.
    - `kubectl logs <pod>` para mostrar los logs de un contenedor en un pod.
    - `kubectl exec <pod> <command>` para ejecutar un comando en un contenedor de un pod.

## Componentes de Kubernetes en acción

### Deployments

Una configuración de Deployment pide a Kubernetes que cree y actualice las instancias de una aplicación. Tras crear el Deployment, el Master organiza las instancias de aplicación en los nodos disponibles del cluster.

image::KubernetesDeployment.svg[]

Una vez creadas las instancias de aplicación, el *Controlador de Deployment de Kubernetes* monitoriza continuamente las instancias. Si un nodo en el que está una instancia cae o es eliminado, el Controlador de Deployment de Kubernetes sustituye la instancia por otra instancia en otro nodo disponible del cluster.

Esta funcionalidad de _autocuración_ de las aplicaciones supone un cambio radical en la gestión de las aplicaciones. Esta característica de recuperación de fallos mediante la creación de nuevas instancias que reemplazan a las defectuosas o desaparecidas no existía antes de los orquestadores.

Al crear un Deployment se especifica la imagen del contenedor que usará la aplicación y el número de réplicas que se quieren mantener en ejecución. El número de réplicas se puede modificar en cualquier momento actualizando el Deployment.

#### Despliegue de una aplicación

Podemos ejecutar una aplicación con `kubectl run` indicando el nombre que se dará al Deployment y el nombre de la imagen (Docker) usada para la aplicación.

[source, bash]
----
$ kubectl run jsonproducer --image=ualmtorres/jsonproducer:v0 --port 80 <1>

deployment.apps/jsonproducer created
----
<1> El puerto hace referencia al puerto que usa la aplicación original para servir su contenido.

Esto ha hecho que el Master haya buscado un nodo para ejecutar la aplicación, haya programado la ejecución de la aplicación en ese nodo y haya configurado el cluster para programar la ejecución de otra instancia cuando sea necesario.

[NOTE]
====
Para imágenes que no estén en Docker Hub se pasa la URL completa del repositorio de imágenes.
====

Para obtener los Deployments disponibles

[source, bash]
----
$ kubectl get deployments

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           8s
----

Para poder acceder a la aplicación deberemos primero exponerla en el cluster de Kubernetes. Más adelante veremos los detalles. Por ahora, basta con ejecutar el comando siguiente, el cual creará un _servicio_ asociado a nuestro Deployment para poder acceder a la aplicación. 

[source, bash]
----
$ kubectl expose deployment jsonproducer --type=NodePort

service/jsonproducer exposed
----

Para ver la ejecución de la aplicación, pediremos a Minikube que nos muestre el _servicio_ con el comando

[source, bash]
----
$ minikube service jsonproducer
----

Esto abrirá un navegador y el resultado del servicio es un JSON similar a este:

[source, json]
----
{"nombre":"manolo"}
----


### Pods

Al crear el Deployment anterior, Kubernetes creó un Pod para ejecutar una instancia de la aplicación. Un Pod es una abstracción de Kubernetes que representa un grupo de uno o más contenedores de una aplicación y algunos recursos compartidos de esos contenedores (p.e. volúmenes, redes)

[NOTE]
====
Un ejemplo de pod con más de un contenedor lo encontramos en lo que se denominan _sidecars_. Ejemplos de sidecar los encontramos en aplicaciones que registran su actividad en un contenedor (sidecar) dentro del mismo pod y publican la actividad en una aplicación que monitoriza el cluster. Otro ejemplo de sidecar es el de un contenedor sidecar que proporciona un certificado SSL para comunicación https al contenedor de la aplicación. Otro ejemplo más lo podemos encontrar en un sidecar que actúa como volumen.
====

Los contenedores de un pod comparten una IP y un espacio de puertos, y siempre van juntos y se despliegan juntos en un nodo. La figura siguiente ilustra varias configuraciones de pods: Un pod con un contenedor, un pod con un contenedor y un volumen, un pod con dos contenedores que comparten un volumen y un pod con varios contenedores y varios volúmenes.

image::KubernetesPod.svg[]

Los pods son la unidad atómica de Kubernetes. Al crear un despliegue en Kubernetes, el Deployment crea Pods con contenedores en su interior. Cada pod queda ligado a un nodo y sigue allí hasta que se finalice o se elimine. En caso de fallo del nodo se planifica la creación de sus pods en otros nodos disponibles del cluster. 

[IMPORTANT]
====
Los pods son efímeros, por lo que su almacenamiento desaparece al eliminar el pod. Por este motivo es necesario saber utilizar almacenamiento externo para que los datos persistan. El almacenamiento se tratará en otra sección de este tutorial.
====


#### Creación de un pod para MongoDB mediante un archivo de manifiesto

Los pods, al igual que otros recursos de Kubernetes (replicasets, volúmenes, ...) se pueden crear sobre la marcha con el CLI indicando la imagen a partir de la que se crean, o se pueden crear a partir de archivos de manifiesto. Estos archivos de manifiesto se escriben en sintaxis https://yaml.org/[YAML,window=_blank] y representan una forma declarativa de definir los recursos del cluster Kubernetes. 

Para ilustrar cómo crear un pod, veremos cómo crear uno sencillo para MongoDB a partir de un archivo de manifiesto. Para ir familiriarizándonos con Kubernetes, probaremos también con unos comandos básicos para mostrar información, mostrar los logs, redirección de puertos

. Creación del manifiesto YAML 
+
Archivo `mongodb-basico.yaml`
+
[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: mongodb 
spec:
  containers:
  - image: mongo
    name: mongodb
----

+
. Despliegue del manifiesto para crear el pod
+
[source, bash]
----
$ kubectl apply -f mongodb-basico.yaml
----

+
. Inicio de sesión SSH en el pod
+
[source, bash]
----
$ kubectl exec -it mongodb /bin/bash
----

+
. Mostrar información del pod
+
[source, bash]
----
$ kubectl describe pod mongodb
----

+
. Mostrar los logs del pod
+
[source, bash]
----
$ kubectl logs mongodb
----

+
. Redirección del puerto del pod a un puerto local (establece un túnel SSH entre nuestro equipo y el pod con los puertos indicados)
+
[source, bash]
----
$ kubectl port-forward mongodb 27017:27017
----

+
[NOTE]
====
Para poder probar el comando anterior de la redirección de puertos necesitaremos disponer de un cliente MongoDB instalado en nuestro equipo.
====

+
. Eliminación del pod

+
[source, bash]
----
$ kubectl delete -f mongodb-basico.yaml
----

### Nodos

Los pods se ejecutan en un Nodo. Un nodo es una máquina _worker_ (física o virtual) del cluster. Los nodos están gestionados por el Master. Un Nodo puede contener muchos pods.

image::KubernetesNode.svg[]

Cada Nodo ejecuta al menos:

* Kubelet, un proceso que se encarga de la comunicación entre el nodo y el Master. Gestiona los pods y los contenedores que se están ejecutando en el nodo.
* Un motor de contenedores, como Docker, que se encarga de la descarga de imágenes de un registro y de ejecutar la aplicación.

### Servicios

Se dice que en Kubernetes los pods son mortales o efímeros. Cuando un nodo desaparece (bien por un error o por una desconexión), los contenedores que están en el nodo también se pierden. A continuación, un _ReplicaSet_ se encarga de devolver el cluster al estado deseado y organiza la creación de nuevos pods en otros nodos disponibles para mantener funcionando la aplicación. Las réplicas de los pods han de ser intercambiables y *aunque cada pod en el cluster tenga su propia IP única, Kubernetes reconcialiará los cambios entre los pods para que las aplicaciones sigan funcionando*.

Los servicios en Kubernetes son una abstracción que definen un conjunto lógico de pods y una política de acceso a ellos estableciendo un nombre para acceder a ellos. Esto permite que haya un acoplamiento débil entre pods dependientes. El acceso puede ser interno o externo al cluster. De esta forma, las aplicaciones sólo usarán los nombres de los servicios y no las IP de los pods, ya que éstas nunca son fijas debido a que, por un lado, los pods se crean y se destruyen para mantener el número de réplicas deseado; y por otro lado, un pod puede ser sustituido por otro ante un problema y el nuevo pod tendrá una IP diferente.

.Agrupación de pods en servicios
****
Los pods son etiquetados con metadatos. Estos metadatos posteriormente son usados por otros objetos Kubernetes (p.e. ReplicaSet, Deployment) para seleccionar los pods y crear una unidad lógica (p.e. todas las réplicas de un contenedor de frontend)

La figura siguiente ilustra como un servicio agrupa mediante el *selector* `app:ngnix` a aquellos pods que están etiquetados con `app:ngnix`.

image::podlabels.png[]

[source,yaml]
----
apiVersion: apps/v1 
kind: Deployment 
metadata: 
  name: nginx
  labels: 
    app: nginx
spec: 
  replicas: 2 
  selector: 
    matchLabels:
      app: nginx 
  template: 
    metadata:
      labels: 
        app: nginx
    spec:
      containers:
      - name: webcontainer 
        image: nginx 
        ports:
        - containerPort: 80 
----

Al desplegar este deployment se crearán dos pods, que quedarán agrupados por la coincidencia entre el selector que pide el deployment (`app: nginx`) y la etiqueta con los que son creados los pods (`app: nginx`).

[source, bash]
----
$ kubectl apply -f ngnix.yaml
----

Si ahora vemos los detalles del deployment en el dashboard de minikube veremos que los dos pods de nginx creados están agrupados lógicamente en el deployment `ngnix`. Esta información está realmente en el objeto ReplicaSet creado por el Deployment.

image::ReplicaSetPods.png[]
****


Cada pod tiene una dirección IP única, pero esa IP no se expone fuera del cluster sin lo que se denomina un Servicio. Los servicios pemiten que las aplicaciones reciban tráfico. En función del ámbito de la exposición del servicio tenemos:

* ClusterIP: El servicio recibe una IP interna a nivel de cluster y hace que el servicio sólo sea accesible a nivel de cluster.
* NodePort: Expone el servicio fuera del cluster concatenando la IP del nodo en el que está el pod y un número de puerto entre 30000 y 32767, que es el mismo en todos los nodos
* LoadBalancer: Crea en cloud, si es posible, un balanceador externo con una IP externa asignada.
* ExternalName: Expone el servicio usando un nombre arbitrario (especificado en `externalName`)

image::KubernetesService.svg[]

Los servicios enrutan el tráfico entre los pods proporcionando una abstracción que permite que los pod mueran y se repliquen sin impactar en la aplicación. 

[NOTE]
====
El descubrimiento y enrutado entre pods dependientes (p.e. frontend y backend) son gestionados por los Servicios. Los servicios agrupan a sus pods usando etiquetas y selectores. Los servicios usan selectores y los pods son creados con etiquetas. Su emparejamiento por valores coincidenetes es lo que agupa los pods en un servicio. 
====

Las etiquetas son pares clave-valor y tienen usos muy variados:

* Seleccionar los objetos de un despliegue
* Diferenciar entre objetos de desarrollo, prueba y producción
* Distinguir entre versiones

image::KubernetesLabels.svg[]

En la figura se observa cómo el selector de etiquetas usado en los Deployment sirve para agrupar los pods que conforman un servicio, ya que cada pod contiene la misma etiqueta usada en el selector del Deployment al que pertenece.

Las etiquetas se pueden configurar durante la creación o en cualquier momento posterior.

[TIP]
====
Prueba a editar un pod en el dashboard de kubernetes cambiándole la etiqueta (p.e. `app:apache`). Esto hará que ese pod salga del ReplicaSet al que pertencía y se cree un nuevo pod etiquetado con `app:nginx` para mantener la especificación del Deployment, que exigía tener 2 réplicas.
====

#### Ejemplo. Creación de un servicio

Anteriormente, en la sección <<Despliegue de una aplicación>> creamos una aplicación de ejemplo que generaba un JSON de prueba. A modo de recordatorio, hicimos lo siguiente:

1. Crear un Deployment a partir de la imagen `ualmtorres/jsonproducer:v0` de Docker Hub con el comando 

+
[source, bash]
----
$ kubectl run jsonproducer --image=ualmtorres/jsonproducer:v0 --port 80
----

+
Podemos consultar el Deployment existente con el comando siguiente. Si por cualquier motivo no se dispone del Deployment, basta con ejecutar el comando anterior para crearlo.

+
[source, json]
----
$ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           17m
----

+
Este Deployment habrá creado un pod que estará ejecutando la aplicación disponible de la imagen utilizada. Podemos ver los pods disponibles con el comando 

+
[source, bash]
----
$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          23m
----

2. Crear un servicio para poder exponer la aplicación al exterior. Concretamente usamos un servicio de tipo NodePort, lo que nos sirve la aplicación concatenando la IP del nodo donde está el pod y un puerto aleatorio. El servicio lo creamos con  

+ 
[source, bash]
----
$ kubectl expose deployment jsonproducer --type=NodePort
----

+
Podemos consultar el servicio existente con el comando siguiente. Si por cualquier motivo no se dispone del servicio, basta con ejecutar el comando anterior para crearlo.

+
[source, bash]
----
$ kubectl get services
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
jsonproducer   NodePort    10.99.116.165   <none>        80:30737/TCP   25m <1>
kubernetes     ClusterIP   10.96.0.1       <none>        443/TCP        34d <2>
----
<1> Este es nuestro servicio. En el caso del tutorial, el puerto aleatorio asignado es el 30737
<2> Servicio `kubernetes` creado de forma predetermianda al iniciarse Minikube

+
Podemos acceder el servicio creado con

+
[source, bash]
----
$ minikube service jsonproducer
----

+
image::KubernetesRunningService.png[]

+
Si queremos consultar la información del servicio creado usaremos la opción `describe` de `kubectl` 

+
[source, bash]
----
$ kubectl describe services jsonproducer <1>

Name:                     jsonproducer
Namespace:                default
Labels:                   run=jsonproducer <2>
Annotations:              <none>
Selector:                 run=jsonproducer
Type:                     NodePort
IP:                       10.99.116.165
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  30737/TCP
Endpoints:                172.17.0.5:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
----
<1> Pasamos el nombre de nuestro servicio como parámetro
<2> Etiqueta añadida de forma predeterminada

+
Si ahora consultamos la información del pod de la aplicación veremos que coincide la etiqueta. Recordemos que al introducir el concepto de Servicio se indicó que era una abstracción para agrupar pods y que utilizaba etiquetas para poder reunirlos. He aquí la correspondencia entre la etiqueta del servicio y la etiqueta de los pods del servicio.

[source, bash]
----
$ kubectl get pods <1> 

NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          49m

$ kubectl describe pods jsonproducer-7769d76894-2nzt2 <2>

Name:               jsonproducer-7769d76894-2nzt2
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               minikube/10.0.2.15
Start Time:         Mon, 15 Jul 2019 18:56:20 +0200
Labels:             pod-template-hash=7769d76894
                    run=jsonproducer <3>
Annotations:        <none>
Status:             Running
IP:                 172.17.0.5
Controlled By:      ReplicaSet/jsonproducer-7769d76894 <4>
Containers:
  jsonproducer:
    Container ID:   docker://52e290262984a94da4dd89102b93d80f59c0c4310c303dac67b02884d73fb545
    Image:          ualmtorres/jsonproducer:v0 <5>
...
----
<1> Obtener primero los pods disponibles para poder acceder al pod deseado
<2> Obtener información del pod
<3> Etiqueta coincidente con el selector (etiqueta) del Deployment
<4> ReplicaSet encargado de mantener el número de pods deseados para el Deployment
<5> Imagen base usada para crear el único contenedor de este pod

### Volúmenes

Básicamente, uno volumen es un directorio para datos que es accesible a los contenedores de un Pod y que persiste a los reinicios de un Pod. El medio que se use para el almacenamiento y cómo se comporte ante una eliminación del Pod depende del tipo de volumen que se use.

Para usar un volumen, un Pod especifica el volumen que proporciona al Pod (el campo `.spec.volumes`) y donde montarlo en los contenedores (el campo `.spec.containers.volumeMounts`). Dejamos por ahora el tema de los volúmenes para volver a ellos más adelante cuando usemos archivos de despliegue.

#### ConfigMaps

Los objetos ConfigMap permiten almacenar datos en forma de clave valor y que pueden usarse posteriormente en un despliegue con el fin de parametrizar los despliegues parametrizados y hacerlos más portables.

Usaremos los ConfigMap para almacenar datos no sensibles sobre la configuración. Deben ser datos no sensibles porque los datos se guardan tal cual.

* Creación de un ConfigMap con valores directamente:

+
[source, bash]
----
$ kubectl create configmap datosmtorres --from-literal=nombre=Manuel --from-literal=apellidos=Torres
----

* Creación de un ConfigMap desde archivos:

+
[source, bash]
----
$ kubectl create configmap datosstevemcqueen --from-file=nombre=nombre.txt --from-file=apellidos=apellidos.txt
----

[IMPORTANT]
====
Los archivos que contienen los valores que alimentarán las claves no contendrán caracteres no deseados como espacios o saltos de línea al final.
====

* Obtener los datos de un ConfigMap

+
[source, bash]
----
$ kubectl describe configmap datosmtorres
$ kubectl describe configmap datosstevemcqueen
----

* Eliminar un ConfigMap

+
[source, bash]
----
$ kubectl delete configmap datosmtorres 
$ kubectl delete configmap datosstevemcqueen
----

### Secrets

Los objetos Secret se usan para almacenar información sensible, como contraseñas, tokens OAuth y claves ssh. Colocar esta información en objetos Secret es más seguro que colocarla en texto plano y legible.

Los datos de los objetos Secret no están cifrados. Están codificados en base64 y pueden hacerse visibles fácilmente. Sistemas como https://www.vaultproject.io/[Vault] son usados de forma complementaria para aumentar la seguridad de la información que contienen los Secret.

* Creación de un Secret con valores directamente:

+
[source, bash]
----
$ kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
----

* Creación de un Secret desde archivos:

+
[source, bash]
----
$ kubectl create secret generic my-second-secret --from-file=key1=key1.txt --from-file=key2=key2.txt
----

* Obtener los datos de un Secret:

+
[source, bash]
----
$ kubectl get secret my-secret -o yaml
----

* Decodificación de un Secret:

+
[source, bash]
----
$  echo 'yourEncodedKey' | base64 --decode
----

* Eliminar un Secret:

+
[source, bash]
----
$ kubectl delete secret my-secret
----

### Namespaces

Abstracción de Kubernetes para soportar varios clusters virtuales en un mismo cluster físico. Los namspaces se usan para organizar objetos en un cluster y para proporcionar una forma de dividir los recursos del cluster. Los nombres de los recursos tienen que ser únicos a nivel de namespace, pero no a nivel de cluster.

[TIP]
====
En clusters con varios usuarios los namespaces proporcionan una forma de agrupar los recursos de cada usuario. Además, los administradores pueden establecer cuotas a nivel de namespace limitando a los usuarios la cantidad de objetos que pueden crear y la cantidad de recursos del cluster que pueden consumir (p.e. CPU, memoria).
====

* Crear un Namespace

+
[source,bash]
----
$ kubectl create namespace rrhh
----

* Crear un pod en un namespace diferente

+
[source,bash]
----
$ kubectl run nginxrrhh --image=nginx --port 80 --namespace rrhh
----

* Mostrar los pods de un namespace

+
[source,bash]
----
$ kubectl get pods --namespace rrhh
----

* Cambiar de namespace

+
[source,bash]
----
kubectl config set-context --current --namespace=rrhh
----

* Volver al namespace default 

+
[source,bash]
----
$ kubectl config set-context --current --namespace=default
----

* Eliminar un namespace

+
[source,bash]
----
$ kubectl delete namespace rrhh
----

[CAUTION]
====
Eliminar un namespace elimina el namespace y todos los objetos que contenga, por lo que es una operación muy peligrosa.
====

[NOTE]
====
Si se elimina un namespace estando situado sobre él no se cambia a ningún namespace por lo que habrá que cambiar a uno de los namespace existentes en nuestro sistema
====

[TIP]
====
https://github.com/ahmetb/kubectx[`kubectx` y `kubens`] son dos herramientas que facilitan la gestión de cambios de namespace y de contexto
====

## Escalado de una aplicación

Hasta ahora hemos creado un Deployment que posteriomente ha sido expuesto mediante un Servicio. Como no indicamos número de réplicas, el Deployment creó sólo un Pod para ejecutar la aplicación. Si la demanda aumenta quizá puede llegar a ser necesario aumentar el número de pods de la aplicación. Esto es lo que se conoce como escalado y hace referencia al número de réplicas en un Deployment.

[NOTE]
====
Para escalar un Deployment durante la creación se usa el parámetro `--replicas=<numero-de-replicas>`.
====

Al escalar una aplicación se crearán nuevos pods en los nodos con recursos disponibles e irá aumentando hasta llegar al número de pods deseados. La ejecución de varias instancias trae consigo la distribución del tráfico entre todos los pods del Deployment. De esta tarea se encarga un balanceador de carga que integra el propio Servicio.

[NOTE]
====
Escalar a 0 terminará todos los pods de un Deployment.
====

Una vez que entramos en la dinámica de tener varias instancias de la misma aplicación, se pueden tener actualizaciones en caliente (_rolling updates_) sin suspensión del servicio. Esto lo veremos en la sección <<Actualización de aplicaciones>>.

### Ejemplo de escalado de una aplicación

En primer lugar veremos cuáles eran las condiciones del despliegue de ejemplo que estamos usando.

[source, bash]
----
$ kubectl get deployments

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           68m
----

* `READY` indica el ratio entre los pods deseados y los que están en ejecución.
* `UP-TO-DATE` indica el número de réplicas que están actualizadas para alcanzar el estado deseado.
* `AVAILABLE` indica el número de réplicas disponibles actualmente para los usuarios.

[NOTE]
====
Si no contamos con este deployment porque vamos limpiando el entorno en cada sección, los comandos siguientes vuelven a crear el Deployment `jsonproducer` para continuar el tutorial.

[source, bash]
----
$ kubectl run jsonproducer --image=ualmtorres/jsonproducer:v0 --port 80 <1>
$ kubectl expose deployment jsonproducer --type=NodePort <2>
$ minikube service jsonproducer <3>
----
<1> Crea el Deployment, ReplicaSet y el Pod
<2> Crea el servicio del tipo NodePort. El servicio está accesible mediante la IP del cluster (`kubectl cluster-info`) concatenada al puerto que se haya asignado (`NodePort`)
<3> Pedir minikube que abra un navegador para acceder al servicio `jsonproducer`
====

El comando siguiente escala a 4 réplicas el despliegue de ejemplo (`jsonproducer`)

[source, bash]
----
$ kubectl scale deployments jsonproducer --replicas=4

deployment.extensions/jsonproducer scaled
----

Unos instantes después podremos comprobar que el Deployment ya ha alcanzado el estado deseado.

[source, bash]
----
$ kubectl get deployments

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   4/4     4            4           73m
----

La aplicación sigue disponible sin ningún cambio para el usuario final. Sin embargo, ahora hay 4 réplicas cuyo tráfico es gestionado por un balanceador de carga asociado al servicio.

image::KubernetesRunningService.png[]

La información de las réplicas la podemos obtener consultando el número de pods con el comando siguiente:

[source, bash]
----
$ kubectl get pods

NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          74m
jsonproducer-7769d76894-9xdqw   1/1     Running   0          38s
jsonproducer-7769d76894-nhtl4   1/1     Running   0          38s
jsonproducer-7769d76894-qbvzd   1/1     Running   0          38s
----

Si ahora por cualquier motivo dejase de estar disponible alguno de los nodos en los que se encuentra desplegados los pods de la apliación, o bien dejase de funcionar alguno de los pods, el Controlador de Deployment de Kubernetes se encargaría de organizar la creación de nuevos pods para volver a alcanzar el estado deseado, en nuestro caso 4 réplicas.

Probemos esta funcionalidad eliminando el último pod y comprobando como Kubernetes organiza inmediatamente la creación de otro pod que lo sustituya.

[source, bash]
----
$ kubectl delete pods jsonproducer-7769d76894-qbvzd
pod "jsonproducer-7769d76894-qbvzd" deleted

$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          85m
jsonproducer-7769d76894-9xdqw   1/1     Running   0          12m
jsonproducer-7769d76894-gh7qk   1/1     Running   0          3s <1>
jsonproducer-7769d76894-nhtl4   1/1     Running   0          12m
----
<1> Pod que sustituye al pod eliminado creado automáticamente para mantener el número de réplicas a 4

Por último, si ahora queremos reducir el número de réplicas a 2 bastará con volver a indicarlo al Deployment en el parámetro `replicas` y este será el nuevo estado a alcanzar.

[source, bash]
----
$ kubectl scale deployments jsonproducer --replicas=2
deployment.extensions/jsonproducer scaled

$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-2nzt2   1/1     Running   0          92m
jsonproducer-7769d76894-9xdqw   1/1     Running   0          18m
----

## Actualización de aplicaciones

Para poder realizar actualizaciones sin tener que suspender el servicio mientras se realiza la actualización, Kubernetes proporciona las _rolling updates_, que van actualizando los pods con la nueva versión de la aplicación.

De forma predeterminada, el número de pods que pueden estar no disponibles durante una actualización es 1, aunque esta opción es configurable, ya sea mediante cantidad o porcentaje de pods no disponibles durante la actualización. Además, es posible volver a una versión anterior.

Al igual que ocurre al escalar las aplicaciones, si el Deployment está expuesto, el Service balancerá el tráfico sólo a los pods que estén disponibles durante la actualización.

A continuación se muestra cómo actualizar el Deployment de ejemplo `jsonproducer` con nuevo Deployment con el mismo nombre y una nueva versión de la imagen (`v1`). 

[source, bash]
----
$ kubectl set image deployments jsonproducer jsonproducer=ualmtorres/jsonproducer:v1
----

Al realizar la actualización de la imagen del Deployment, Kubernetes tendrá que descargar la nueva imagen y organizar la creación de los pods en los nodos con recursos disponibles. Mientras se realiza la actualización podremos ver que hay nodos que se están terminando, otros que se están creando y otros que están disponibles.

[source, bash]
----
$ kubectl get pods
NAME                            READY   STATUS              RESTARTS   AGE
jsonproducer-7769d76894-fr7cz   1/1     Running             0          25s
jsonproducer-7769d76894-hfpr7   1/1     Terminating         0          24s
jsonproducer-c76c87f-jwhxq      0/1     ContainerCreating   0          0s
jsonproducer-c76c87f-tmbkk      1/1     Running             0          1s
----

Tras unos instantes, la aplicación dejará de servir totalmente la versión anterior de la aplicación y comenzará a servir la nueva versión. La nueva versión de la aplicación devuelve `Manolo Torres` en lugar de `manolo` en el JSON.

image::KubernetesUpdateImage.png[]

Para deshacer una actualización de una aplicación volviendo a la versión anterior haremos un `rollout undo`. El comando siguiente devuelve a la aplicación a la versión anterior

[source, bash]
----
$ kubectl rollout undo deployments jsonproducer
deployment.extensions/jsonproducer rolled back
----

Tras este comando, el Controlador de Deployment de Kubernetes irá reemplanzando los pods hasta alcanzar el estado deseado. A continuación se ve el estado intermedio mientras se vuelve a la versión anterior.

[source, bash]
----
$ kubectl get pods 
NAME                            READY   STATUS        RESTARTS   AGE
jsonproducer-7769d76894-m22sv   1/1     Running       0          2s
jsonproducer-7769d76894-v6hfv   1/1     Running       0          4s
jsonproducer-c76c87f-jwhxq      0/1     Terminating   0          14m
jsonproducer-c76c87f-tmbkk      0/1     Terminating   0          14m
----

Tras unos instantes, se alcanzará el estado deseado

[source, bash]
----
Caligari:~ manolo$ kubectl get pods
NAME                            READY   STATUS    RESTARTS   AGE
jsonproducer-7769d76894-m22sv   1/1     Running   0          8s
jsonproducer-7769d76894-v6hfv   1/1     Running   0          10s
----

Y la aplicación volverá a mostrar el contenido anterior.

image::KubernetesRunningService.png[]

## Despliegue de aplicaciones mediante archivos YAML

Hasta ahora, las interacción con Kubernetes la hemos hecho sobre la marcha, creando despliegues, servicios, escalado de aplicaciones y demás. Esto nos ha servido familiarizarnos tanto con los objetos básicos de Kubernetes (Pod, ReplicaSet, Deployment, Service, ConfigMap, Secret, Namespace, volúmenes, ...), como con operaciones habituales (escalado, actualización de versiones, ...). Sin embargo, esta no es la forma habitual. Esta forma de uso de Kubernetes está más orientada a la creación de tareas puntuales. En cambio, cuando se trata de operaciones que queremos que sean repetibles, la forma de operar consiste en crear archivos YAML especificando el objeto que se quiere crear en Kubernetes (espacio de nombres, despliegue, servicio, ...). Una vez creados estos archivos, se usará `kubectl` para cargarlos/desplegarlos en Kubernetes.

[NOTE]
====
El uso de archivos para despliegues Kubernetes nos permitirá además beneficiarnos de las ventajas de los sistemas de control de versiones, sometiendo nuestros recursos de Kubernetes al control de versiones, facilidad de distribución y trabajo en equipo.
====

Para ilustrar el despliegue de una aplicación mediante archivos YAML vamos a desplegar una aplicación de ejemplo que consuma del servicio `jsonproducer` creado anteriormente. Se trata de un ejemplo muy sencillo de un entorno frontend-backend con un funcionamiento independiente. Esto, además de desacoplar la presentación del backend, desde el punto de vista de la escalabilidad, permite escaladar backend y frontend de forma independiente.

### Creación del archivo de Deployment 

Un archivo de Deployment proporciona una forma declarativa de creación de Pods y ReplicaSets. En el archivo de Deployment se especifica el estado deseado.

Vamos a crear un archivo de Deployment denominado `json-reader-deployment.yaml`. Este archivo básicamente contiene entre otros el nombre de despliegue, la etiqueta usada para agrupar los pods del servicio, número de réplicas y la imagen usada para crear el contenedor de cada pod.

[source, yaml]
----
apiVersion: apps/v1
kind: Deployment <1>
metadata:
  name: jsonreader <2>
  namespace: default <3>
  labels:
    app: jsonreader <4>
spec:
  revisionHistoryLimit: 2 <5>
  strategy:
    type: RollingUpdate <6>
  replicas: 2 <7>
  selector:
    matchLabels:
      app: jsonreader <8>
  template:
    metadata:
      labels:
        app: jsonreader
    spec:
      containers:
      - name: jsonreader <9>
        image: ualmtorres/jsonreader:v0 <10>
        ports:
        - name: http
          containerPort: 80 <11>
----
<1> Tipo de recurso a desplegar
<2> Nombre del despliegue
<3> Namespace de despliegue
<4> Selector usado para agrupar a los pods del servicio asociado
<5> Número de versiones almacenadas para poder deshacer despliegues fallidos
<6> Tipo de estrategia de actualización
<7> Número de réplicas del despliegue
<8> Selector que define cómo el Deployment encuentra los Pods a gestionar, *que coincide con el definido en la plantilla (template) del pod*
<9> Prefijo usado para los pods
<10> Imagen base para los contenedores de la aplicación
<11> Puerto por el que la aplicación sirve originalmente sus datos

[NOTE]
====
La estrategia de despliegue (`spec.strategy.type`) puede ser `Recreate` o `RollingUpdate`, que es el valor predeterminado.
====

El despliegue se realiza con `kubectl` con el comando siguiente

[source, bash]
----
$ kubectl apply -f json-reader-deployment.yaml
----

Al crear el despliegue, se procederá a descargar la imagen y se pasarán a crear los dos pods indicados para este despliegue. Podemos ver los pods creados con el comando siguiente comprobando que efectivamente se creado los dos pods `jsonreader` que exigía el despliegue.

Podemos ver el despliegue con el comando siguiente

[source, bash]
----
$ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer   1/1     1            1           22h
jsonreader     2/2     2            2           21h
----

También podemos ver los ReplicaSets creados por los despliegues

[source, bash]
----
$ kubectl get rs
NAME                      DESIRED   CURRENT   READY   AGE
jsonproducer-7769d76894   1         1         1       22h
jsonreader-86699d9f94     2         2         2       22h
----

Los pods los podemos ver junto con sus etiquetas con el parámetro `--show-labels`

[source, bash]
----
$ kubectl get pods --show-labels
NAME                            READY   STATUS    RESTARTS   AGE   LABELS
jsonproducer-7769d76894-ss5qh   1/1     Running   1          22h   pod-template-hash=7769d76894,run=jsonproducer
jsonreader-86699d9f94-khfzh     1/1     Running   1          22h   app=jsonreader,pod-template-hash=86699d9f94
jsonreader-86699d9f94-lrvpt     1/1     Running   1          22h   app=jsonreader,pod-template-hash=86699d9f94
----

### Creación del archivo de Servicio

Un Servicio es una abstracción que define una agrupación de Pods y una política de acceso a ellos. El conjunto de Pods al que se dirige un Servicio están determinados por un *selector*.

Vamos a crear un archivo de Servicio denominado `json-reader-service.yaml`. Este archivo básicamente contiene entre otros el nombre de servicio, el tipo del servicio (ClusterIP, NodePort, ...), el puerto de acceso a los pods del desplieguw y el selector que identifica al despliegue con el que se corresponde el servicio creado.

[source, yaml]
----
apiVersion: v1
kind: Service <1>
metadata:
  name: jsonreader <2>
  namespace: default <3>
spec:
  type: NodePort <4>
  ports:
  - name: http
    port: 80 <5>
    targetPort: http
  selector:
    app: jsonreader <6>
----
<1> Tipo de recurso a desplegar
<2> Nombre del servicio
<3> Namespace de despliegue
<4> Tipo de servivio. NodePort hará que el servicio esté disponible en la IP de los nodos en los que estén los pods y un puerto aleatorio entre 30000 y 32767
<5> Puerto en el que los pods están sirviendo su contenido
<6> Etiqueta que tiene que coincidir con la usada en el Deployment

El despliegue se realiza con `kubectl` con el comando siguiente

[source, bash]
----
$ kubectl create -f json-reader-service.yaml
----

El despliegue nos permitirá acceder a la aplicación en un puerto en el rango 30000-32767. En este caso ha tocado el 31976

[source, bash]
----
$ kubectl get services
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
jsonproducer   NodePort    10.105.30.95   <none>        80:30228/TCP   22h
jsonreader     NodePort    10.99.85.2     <none>        80:31976/TCP   22h
kubernetes     ClusterIP   10.96.0.1      <none>        443/TCP        22h
----


Para poder acceder al servicio pediremos a Minikube que nos lo muestre.

[source, bash]
----
$ minikube service jsonreader
----

Esto hará que se abra un navegador con la aplicación `jsonreader` que simplemente lee el JSON y presenta un saludo sencillo.

image::KubernetesServiceReader.png[]

También podemos usar el Kubernetes Dashboard para mostrar información de interés sobre este despliegue, viendo como de Deployment de `jsonreader` se ha incorporado a la lista de despliegues disponibles en el cluster, así como los pods, ReplicaSets y servicios, como muestran las figuras siguientes.

image::KubernetesDashboardJSON1.png[]

image::KubernetesDashboardJSON2.png[]

## ConfigMaps

Variables de entorno
[source, yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: configmap-env
spec:
  containers:
  - name: php-apache
    image: php:7-apache
    env:
      - name: NOMBRE
        value: Manolo
      - name: APELLIDOS
        value: Torres
----

ConfigMap
[source, bash]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfigmap
data:
  nombre: Manolo
  apellidos: Torres
---
apiVersion: v1
kind: Pod
metadata:
  name: configmap-example
spec:
  containers:
  - name: configmap-cm
    image: php:7-apache
    env:
      - name: NOMBRE
        valueFrom:
          configMapKeyRef:
            name: myconfigmap
            key: nombre
      - name: APELLIDOS
        valueFrom:
          configMapKeyRef:
            name: myconfigmap
            key: apellidos
----

Secret

## Apéndice. Cheat Sheet

### Comandos Minikube

* `minikube version`
* `minikube start`
* `minikube dashboard`
* `minikube service <nombre-servicio>`
* `minikube delete`


### Comandos `kubectl`

* `kubectl version`
* `kubectl cluster-info`
* `kubectl get nodes|deployments|services|pods [--show-labels]` 
* `kubectl run <deployment> --image=<image> --port=<container-port>`
* `kubectl expose deployment <deployment>> --type=NodePort`
* `kubectl describe pods|deployments|services <resource>`
* `kubectl scale deployments <deployment> --replicas=<number-of-replicas>`
* `kubectl delete pods|deployments|services <resource>`
* `kubectl set image deployments <deployment> <deployment>=<image>`
* `kubectl rollout undo deployments <deployment>`
* `kubectl apply -f <filename-or-URL>`
* `kubectl logs <pod>`
* `kubectl exec <pod> <command>`

### Herramientas interesantes

* https://github.com/ahmetb/kubectx[`kubectl` y `kubens`]: Cambio de namespace contexto

cloud_provider: 
  name: "openstack"
  openstackCloudProvider: 
    block_storage: 
      ignore-volume-az: true
      trust-device-path: false
    global: 
      auth-url: "http://192.168.64.12:5000/v3/"
      domain-name: "default"
      tenant-name: "mtorres"
      username: "mtorres"
      password: "xxx"
    load_balancer: 
      create-monitor: false
      manage-security-groups: false
      monitor-max-retries: 0
      use-octavia: false
    metadata: 
      request-timeout: 0

### Contextos

El archivo de contextos

Disponible en `~/.kube/config`

[source, bash]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/manolo/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
current-context: ""
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /Users/manolo/.minikube/client.crt
    client-key: /Users/manolo/.minikube/client.key
----

Obtener los contextos

[source, bash]
----
$ kubectl config get-contexts
CURRENT   NAME            CLUSTER         AUTHINFO     NAMESPACE
          minikube        minikube        minikube 
----

Añadir un contexto nuevo

Obtener los datos de conexión a Rancher desde 

image::RancherKubeconfig.png[]

Ahí aparecen los datos de conexión al cluster. Ahí se encuentran los datos que tenemos que copiar en el archivo `~/.kube/config`


image::KubeconfigCluster.png[]
image::KubeconfigUser.png[]

Editamos el archivo el archivo `~/.kube/config` y debería quedar algo así
[source, bash]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /Users/manolo/.minikube/ca.crt
    server: https://192.168.99.100:8443
  name: minikube
- cluster: <1>
    certificate-authority-data: XXXXXXXXXXXXXXXXXXX
  name: produccion-ci
contexts:
- context:
    cluster: minikube
    user: minikube
  name: minikube
- context: <2>
    cluster: produccion-ci
    namespace: mtorres
    user: user-XXXXXX
  name: produccion-ci
current-context: ""
kind: Config
preferences: {}
users:
- name: minikube
  user:
    client-certificate: /Users/manolo/.minikube/client.crt
    client-key: /Users/manolo/.minikube/client.key
- name: user-XXXXX <3>
  user:
    token: XXXXXXXXXXXXXXXXXXXXX
----
<1> Datos del cluster de Rancher
<2> Datos del nuevo contexto
<3> Datos del usuario

Usar un contexto

[source, bash]
----
$ kubectl config use-context produccion-ci

Switched to context "produccion-ci".
----

Si ahora consultamos los contextos, veremos que el contexto activo es `produccion-ci`. Por tanto, todas las operaciones que hagamos con `kubectl` a partir de ahora se dirigirán contra ese contexto (cluster-usuario-namespace).

[source, bash]
----
$ kubectl config get-contexts
CURRENT   NAME            CLUSTER         AUTHINFO     NAMESPACE
          minikube        minikube        minikube     
*         produccion-ci   produccion-ci   user-mzmh8   mtorres
----

## Apéndice. Service Mesh con Istio

Los microservicios se han convertido en algo habitual en las aplicaciones cloud actuales. Las arquitecturas de microservicios permiten realizar cambios en un servicio sin tener que volver a desplegar toda la aplicación. Esto permite que los microservicios pueden ser creados por grupos de desarrollo pequeños, creando sus propias herramientas y usando los lenguajes de programación más adecuados, lo que aumenta la productividad y velocidad del proyecto. Básicamente, los microservicios se construyen de forma independiente, se comunican entre sí y permiten el fallo de forma individual sin provocar una caída del funcionamiento de la aplicación completa.

Sin embargo, el desarrollo y la utilización de microservicios supone nuevos desafíos e implica la implementación y la gestión de la comunicación entre ellos. Esta lógica podría ser codificada en cada servicio, lo que aumenta su complejidad y dificultad, y a medida que el proyecto crece, se hace más necesario un _service mesh_.

Un _service mesh_ es una capa complementaria a la aplicación y es responsable de la gestión del tráfico, políticas, certificados y seguridad de los servicios. Así, un _service mesh_ no añade nueva funcionalidad a las aplicaciones. Simplemente, se dedica a sacar fuera de los servicios la lógica de comunicación, abstrayéndola a una capa de infraestructura. Así, con un _service mesh_ los desarrolladores pueden centrarse en el desarrollo de la lógica de negocio y abstraerse de lo demás.

Para ofrecer esta funcionalidad, un _service mesh_ introduce una colección de proxies de red. En un _service mesh_ las peticiones entre los servicios se enrutan a través de estos proxies. Dichos proxies son implementados como sidecars, y se situan en el mismo pod que el servicio al que _sirven_ (tráfico, seguridad, ...). Si imaginamos esta red de sidecars en una capa aparte visualizaremos el _service mesh_.

image::ServiceMesh.png[]

La idea entonces es inyectar un contenedor sidecar especial en el pod de cada microservicio y enrutar todo el tráfico a través de estos sidecars en lugar de a través de los propios microservicios. El controlador del _service mesh_ interactuará con estos proxies y podrá filtrar tráfico, así como aplicar políticas de balanceo, seguridad y limitación de tráfico.

Sin un _service mesh_, cada microservicio debería incluir la lógica de gobierno y de comunicación con otros servicios, lo que añade una complejidad extra al desarrollo del servicio. Además, el disponer de un _service mesh_ en entornos normalizados permite tratar de forma estándar el problema del tráfico, así como la gestión de políticas, seguridad y certificados entre servicios, independientemente de la plataforma en la que estén desplegadas nuestras aplicaciones.

En el contexto de _service mesh_ se suelen tratar con varioss proyectos. Los más habituales son los siguientes:

* https://istio.io/[Istio,window=_blank]: _Service mesh_ que permite conectar, asegurar, controlar y observar servicios. 

* https://www.kiali.io/[Kiali,window=_blank]: Extiende estas características de gestión del tráfico incorporando observabilidad y visualización de servicios de la red. Kiali ofrece una forma sencilla de ver la topología de un _service mesh_ y observar cómo interactúan los servicios.

* https://www.jaegertracing.io[Jaeger,window=_blank]: Se encarga del tracing y permite analizar una petición desde el principio hasta el final, y comprobar las latencias de cada una de las etapas por las que va pasando.

* https://prometheus.io/[Prometheus,window=_blank]: Sistema de monitorización y alertas que almacena en forma de series temporales la actividad del _mesh_ (peticiones, volumen de descarga, tiempos de resolución, errores, ...)

image::ServiceMeshKialiJaeger.png[]

### Istio

Istio viene a incorporarse al vocabulario marinero y ballenero del ecosistema de Docker y Kubernetes. Istio es una palabra griega que significa _navegar_.

[NOTE]
====
Istio está disponible en Rancher desde la versión 2.3.0-alpha5. Basta activarlo en el menú `Tools`. Pedirá si se quiere realizar la inyección automática de sidecars en un _namespace_. Esto hará que se cree un sidecar en cada pod del _namespace_ para el `Istio-proxy`. *Este proxy intercepta todo el tráfico al microservicio del pod y asumirá la gestión del enrutado, la selección de versiones, el registro de actividad y tráfico, y el control de acceso*. Por tanto, en cada _namespace_ en el que quede activado Istio se tendrá configurada la etiqueta `istio-injection=enabled`. No obstante, también es posible activarlo de forma manual, lo que exigiría un reinicio de los servicios, despliegues y otros objetos Kubernetes para que se active el funcionamiento de Istio.

====

La figura siguiente ilustra una aplicación sin Istio. En ella, cada microservicio es el responsable de implementar la funcionalidad de _discovery_, balanceo, resilencia, métricas y trazado.

image::beforeIstio.jpg[]

La figura siguiente ilustra cómo en las aplicaciones basadas en Istio los pods están formados por dos contenedores: el contenedor propio del microservicio y el del sidecar. Al sidecar se le delegan las tareas de _discovery_, balanceo, resilencia, métricas y trazado, lo que facilita el desarrollo de los microservicios.

image::afterIstio.jpg[]

Istio ofrece una forma declarativa, mediante la creación de manifiestos YAML, de gestión del tráfico, enrutado selectivo de peticiones (en lugar del round robin que ofrece Kubernetes), diferentes tipos de despliegue (_canary, A/B, blue/green_), resilencia a nivel de red (con opciones de _retry_, _timeout_), control de acceso, observación de microservicios distribuidos comprendiendo los flujos y trazas y pudiendo ver las métricas importantes de forma inmediata, inyección de caos para poner a prueba la resilencia de aplicaciones y servicios, por citar algunas de sus funcionalidades destacadas.

Para activar el uso de Istio en un namespace (p.e. `default`) se haría con 

[source, bash]
----
kubectl label namespace default istio-injection=enabled
----


### Arquitectura de Istio

Istio consta de un plano de control y un plano de datos. El plano de datos está formado por proxies que se integran en los pods de la aplicación. Usando el patrón del sidecar, cada instancia de la aplicación tendrá su proxy dedicado a través del cual pasa todo el tráfico antes de llegar a la aplicación. Estos proxies individuales son gestionados individualmente por Istio para enrutar, filtrar y aumentar el tráfico según sea necesario.

image::istioArchitecture.jpg[]

Además, Istio permite realizar deciciones de enrutado en función de las cabeceras HTTP (p.e. tipo de navegador, usuario, ...)

image::istioCanary.jpg[]

[NOTE]
====
Algo a tener en cuenta es que los componentes del plano de control son aplicaciones sin estado, lo que favorece que puedan escalar horizontalmente. Todos los datos están almacenados en _etcd_ como descricpciones personalizadas de recursos Kubernetes.
====

Sin embargo, toda esta funcionalidad tiene un coste sobre la infraestructura. Cuando mayor sea el cluster, mayor será la carga añadida al sistema. Cada sidecar consume bastante RAM (unos 350Mb). Además, añade una latencia de unos 10 ms a cada petición.


### Control de tráfico y técnicas de despliegue 

* Despliegue _canary_: Se despliega en producción una nueva versión del código, pero sólo se dirige a ella una parte del tráfico. A la nueva versión quizá sólo tengan acceso clientes de prueba, empleados, usuarios de iOS, etcétera. Una vez desplegado el canario, éste se monitoriza para comprobar la posible existencia de excepciones, comportamiento no satisfactorio, bajada del rendimiento, y demás. Si el canario no muestra indicios de que presente problemas, se puede ir aumentando paulatinamente el tráfico hacia él. En cambio, si presenta un comportamiento inaceptable, se puede retirar fácilmente de producción.

* Control del tráfico: Se pueden especificar reglas de enrutado que controlen el tráfico a un conjunto de pods. En concreto, Istio usa los recursos `DestinationRule` y `VirtualService` en forma de manifiestos YAML para describir estas reglas.
    - `DestinationRule`: Define grupos (_subsets_) de pods. Normalmente definiremos un _subset_ para cada servicio y el _subset_ estará formado por cada una de las versiones que se pueden utilizar del servicio. A continuación se muestra un fragmento YAML con la definición de una `DestinationRule` que define dos versiones posibles a las que enrutar tráfico con Istio.
    
+
[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: jsonproducer
spec:
  host: jsonproducer
  subsets:
  - name: v0
    labels:
      version: v0
  - name: v1
    labels:
      version: v1
----

    - `VirtualService` dirige el tráfico a un _subset_, y lo puede hacer basándose en porcentajes, cabeceras, direcciones IP, por citar algunas. La selección de pods afectados es similar al modelo de selectores utilizado por Kubernetes para selección basada en etiquetas (_labels_). A continuación se muestra un fragmento YAML con la definición de un `VirtualService` para aplicar un enrutado del 80% de las peticiones a la versión `v0` de un microservicio y el 20% a la versión `v1`.
+
[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonproducer
spec:
  hosts:
  - jsonproducer
  http:
  - route:
    - destination:
        host: jsonproducer
        subset: v0
      weight: 80
    - destination:
        host: jsonproducer
        subset: v1
      weight: 20
----

+
Este comportamiento del enrutado no es sólo para el tráfico de entrada externo. Es para toda la comunicación inter-servicio en el _service mesh_. Así, si hubiese servicios desplegados en Kubernetes, pero que no sean parte del _mesh_, dichos servicios no estarían afectados por estas reglas y se regirían por las reglas de balanceo de Kubernetes (round-robin).

* _Dark launch_: Se trata de un despliegue a producción que no es visible a los clientes. En este caso Istio permite duplicar (_mirror_) el tráfico a una versión de la aplicación y ver cómo se comporta respecto a la versión del pod en producción. De esta forma se están realizando peticiones en las condiciones de producción al nuevo servicio sin afectar al tráfico de la versión en producción. No obstante, hay que tener una consideración especial con los servicios que traten con datos o estén vinculados a otros servicios, para no introducir duplicados, provocar inconsistencias y otros problemas derivados de la duplicación de peticiones.

### Técnicas de resilencia

* _Circuit breaker_: Determina el número máximo de peticiones que puede soportar un pod. Pasado ese valor no admite más hasta que se recupere.
* _Pool ejection_: Saca de un nodo a un pod que esté dando fallos creando un nuevo pod que los sustituya en otro nodo.
* _Retries_: Reenvía la petición a otro pod al encontrar un caso de _circuit breaker_ o _pool rejection_.

### Caso práctico

Para no perdernos en los detalles usaremos un ejemplo muy sencillo con dos servicios: uno que produce datos y otro que los presenta. Podríamos ver este ejemplo como un ejemplo muy reducido de backend y frontend.

El servicio que genera datos se denomina `jsonproducer` y genera un documento JSON con un único elemento `nombre` y un valor asociado (p.e. `{"nombre": "manolo"}`). De este servicio se cuenta con dos versiones, cada una con su imagen Docker correspondiente. La primera versión (`v0`) devuelve el elemento JSON `{"nombre": "manolo"}`. La segunda versión (`v1`) devuelve el elemento JSON `{"nombre": "Manuel Torres"}`

El servicio que consume datos se denomina `jsonreader` y usará los datos leídos de `jsonproducer` para presentarlos al usuario en forma de saludo, mostrando `Hola` seguido del nombre leído del JSON devuelto por la versión de `jsonproducer` usada (`Hola manolo` cuando use `v0` y `Hola Manuel Torres` cuando use `v1`).

#### Creación de todos los servicios 

En primer lugar vamos a desplegar en el cluster de Kubernetes todos los recursos (`Service` y `Deployment`) con todas sus versiones correspondientes. Posteriormente, con Istio controlaremos el tráfico que se dirige a cada versión desplegada. En nuestro ejemplo se definirán dos objetos `Deployment`, uno para cada una de las versiones del `jsonproducer` (`v0` y `v1`) que quedarán desplegadas en el cluster.

Cada despliegue incorpora en los metadatos el nombre que le queremos dar, así como unas etiquetas con su versión, que le permitirán ser seleccionado posteriormente cuando se definan los _servicios virtuales_. Además, en la `spec` del despliegue se usarán etiquetas en `matchLabels` que permitirán más adelante a Istio distinguir los pods correspondientes a cada despliegue.

En este ejemplo usaremos dos versiones (dos recursos `Deployment`) del `jsonproducer`. La primera está basada en la imagen `ualmtorres/jsonproducer:v0` que devuelve `{"nombre": "manolo"}`. La segunda está basada en la imagen `ualmtorres/jsonproducer:v1` que devuelve `{"nombre": "Manuel Torres"}`. Con este ejemplo tan sencillo nos bastará para ver a Istio en acción controlando el tráfico. 

El manifiesto siguiente (https://gist.githubusercontent.com/ualmtorres/b651f3c73aa87376752c606e3747b151/raw/9f289b6500b75d93f22b4b36c63c6e3f409f1ccf/greeter.yaml[`greeter.yaml`,window=_blank]) configura varios recursos Kubernetes:

* Dos servicios (`jsonproducer`  y `jsonreader`).
* Dos `Deployment` de `jsonproducer`, correspondientes a las dos versiones  (`jsonproducer-v0` y `jsonproducer-v1`).
* `Deployment` de `jsonreader` (`jsonreader-v0`).

[source, bash]
----
#########################################################
# jsonproducer service
#########################################################
apiVersion: v1
kind: Service
metadata:
  name: jsonproducer
  labels:
    app: jsonproducer
    service: jsonproducer
spec:
  ports:
  - port: 80
    name: http
  selector:
    app: jsonproducer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jsonproducer-v0 <1>
  labels:
    app: jsonproducer
    version: v0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jsonproducer
      version: v0 <2>
  template:
    metadata:
      labels:
        app: jsonproducer
        version: v0
    spec:
      containers:
      - name: jsonproducer
        image: ualmtorres/jsonproducer:v0 <3>
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jsonproducer-v1 <4>
  labels:
    app: jsonproducer
    version: v1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jsonproducer
      version: v1 <5>
  template:
    metadata:
      labels:
        app: jsonproducer
        version: v1
    spec:
      containers:
      - name: jsonproducer
        image: ualmtorres/jsonproducer:v1 <6>
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
---
###########################################################
# jsonreader services
###########################################################
apiVersion: v1
kind: Service
metadata:
  name: jsonreader
  labels:
    app: jsonreader
    service: jsonreader
spec:
  ports:
  - port: 80
    name: http
  selector:
    app: jsonreader
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jsonreader-v0
  labels:
    app: jsonreader
    version: v0
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jsonreader
      version: v0
  template:
    metadata:
      labels:
        app: jsonreader
        version: v0
    spec:
      containers:
      - name: jsonreader
        image: ualmtorres/jsonreader:v0
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
---
----
<1> Versión `v0` del servicio
<2> Selector para determinar los pods asociados a la versión `v0` del servicio
<3> Imagen `v0` del servicio
<4> Versión `v1` del servicio
<5> Selector para determinar los pods asociados a la versión `v1` del servicio
<6> Imagen `v1` del servicio

Lo aplicaremos en nuestro cluster con 

[source, bash]
----
$ kubectl apply -f https://gist.githubusercontent.com/ualmtorres/b651f3c73aa87376752c606e3747b151/raw/9f289b6500b75d93f22b4b36c63c6e3f409f1ccf/greeter.yaml
----

Después, podemos consultar los servicios y _deployments_ configurados

[source, bash]
----
$ kubectl get services
NAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
jsonproducer                               ClusterIP   10.43.88.162    <none>        80/TCP           40s
jsonreader                                 ClusterIP   10.43.106.61    <none>        80/TCP           40s

$ kubectl get deployments
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
jsonproducer-v0   1/1     1            1           40s
jsonproducer-v1   1/1     1            1           40s
jsonreader-v0     1/1     1            1           40s
----

[TIP]
====
De cara a poder probar los distintos escenarios que vamos a desarrollar, sería conveniente crear el Ingress en Rancher para los servicios `jsonproducer` y `jsonreader` y que nos genere automáticamente un hostname `.xip.io` para cada uno de ellos. 
====

#### Creación de los _subsets_ mediante `DestinationRule`

A continación vamos a definir todas las versiones de un servicio que vamos a tener elegibles en el cluster de Kubernetes y que posteriormente serán seleccionadas o usadas cuando pasemos a controlar el tráfico a cada versión. Las `DestinationRule` se usan para definir las distintas instancias o versiones disponibles que se pueden usar de cada servicio. Cada servicio tendrá su `DestinationRule` con lo siguiente:

* `metatada.name`: Nombre.
* `spec.host`: Host contra el que se lanzará este servicio. Puede ser un nombre DNS (admite _wildcards_) o un nombre de servicio válido en nuestra aplicación.
* `spec.subsets`: Lista de versiones de servicios a configurar. Cada versión tendrá su nombre (`name`) y usará una etiqueta (p.e. `labels.version: v0`) para emparejarse con los pods de su versión de acuerdo a lo definido en el selector `matchLabels` del objeto `Deployment` del manifiesto del apartado anterior.


El manifiesto siguiente (https://gist.githubusercontent.com/ualmtorres/b651f3c73aa87376752c606e3747b151/raw/9f289b6500b75d93f22b4b36c63c6e3f409f1ccf/greeter.yaml[`destination-rule-all.yaml`,window=_blank]) corresponde a los dos recursos `DestinationRule` que vamos a crear, uno para cada servicio, `jsonproducer`  y `jsonreader`, respectivamente. Cada `DestinationRule` incluye los _subsets_ o versiones de recursos `Deployment` de cada servicio. En este caso, indicamos que `jsonreader` consta sólo de un _subset_  al que llegaremos a través de la versión `v0` del servicio (_host_) `jsonreader`. En cambio, `jsonproducer` consta de dos _subset_ , a los que llegaremos a través de las versiones `v0` o `v1` del servicio (_host_) `jsonproducer`. 

[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: jsonreader
spec:
  host: jsonreader
  subsets:
  - name: v0
    labels:
      version: v0
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: jsonproducer
spec:
  host: jsonproducer <1>
  subsets:
  - name: v0
    labels:
      version: v0
  - name: v1 <2>
    labels:
      version: v1 <3>
---
----
<1> Nombre que le damos a nuestro host y que luego será usado por los _servicios virtuales_ para dirigir el tráfico a una versión concreta de las definidas en los `subsets`. *Este nombre debe coincidir con los nombres de servicio usados en el código de la aplicación*
<2> Nombre dado a esta versión del servicio
<3> Etiqueta usada para seleccionar los pods a los que corresponde esta versión. Se emparejarán los pods que tengan `version: v1` en su `MatchingLabels`

Lo aplicaremos en nuestro cluster con 

[source, bash]
----
$ kubectl apply -f https://gist.githubusercontent.com/ualmtorres/440e04403d2ec662bda2a9cc29721a75/raw/37542b7886916efe9f306c6d9c48bafef91b335d/destination-rule-all.yaml
----

Después, podemos consultar las `DestinationRule` configuradas

[source, bash]
----
$ kubectl get destinationrules
NAME           HOST           AGE
jsonproducer   jsonproducer   40s
jsonreader     jsonreader     40s
----

#### Creación de los servicios virtuales para el control del tráfico a versiones específicas

Por último, crearemos los `VirtualService` para indicarle a Istio la versión concreta de cada microservicio desplegado a la que queremos desviar el tráfico. Este recurso es el que Istio usará para configurar los proxies que controlarán el tráfico en el _mesh_.

Con los servicios virtuales conseguimos poner en marcha la capa complementaria a la aplicación que controlará su tráfico. Esto nos permite usar y cambiar a versiones concretas, derivar un porcentaje del tráfico a versiones determinadas (p.e. para despliegues `canary`), tener versiones diferentes para usuarios diferentes, control de tráfico basado en CIDR, y demás. 


[NOTE]
====
Con Istio podremos cambiar el enrutado a unos servicios u otros de forma dinánica. Basta con aplicar otro manifiesto con los nuevos valores de enrutado de los `VirtualService` que seleccionen las versiones correspondientes, el porcentaje de derivación de tráfico entre versiones que coexistan, y demás. El _mesh_ cambiará de acuerdo a las nuevas especificaciones.
====

El manifiesto siguiente (https://gist.githubusercontent.com/ualmtorres/e900fc85856fec3d918575e6e3d91d29/raw/35d799ca518f8e5ad76112ecf575a6b8e62f3ff3/all-v0.yaml[`all-v0.yaml`,window=_blank]) define un servicio virtual para cada servicio de nuestra aplicación. En este ejemplo cada servicio virtual usa la versión `v0` de los `Deployment` desplegados en el cluster.

[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonreader
spec:
  hosts:
  - jsonreader
  http:
  - route:
    - destination:
        host: jsonreader
        subset: v0
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonproducer
spec:
  hosts:
  - jsonproducer <1> 
  http:
  - route:
    - destination:
        host: jsonproducer <2>
        subset: v0 <3>
---
----
<1> Nombre DNS (admite prefijos _wildcard_) o nombres de servicios del _mesh_
<2> Servicio al que se quiere dirigir el tráfico
<3> Versión a la que se quiere dirigir el tráfico

Lo aplicaremos en nuestro cluster con 

[source, bash]
----
$ kubectl apply -f https://gist.githubusercontent.com/ualmtorres/e900fc85856fec3d918575e6e3d91d29/raw/35d799ca518f8e5ad76112ecf575a6b8e62f3ff3/all-v0.yaml
----

Después, podemos consultar los servicios virtuales configurados

[source, bash]
----
$ kubectl get virtualservices
NAME           GATEWAYS             HOSTS            AGE
jsonproducer                        [jsonproducer]   40s
jsonreader                          [jsonreader]     40s
----

Si lanzamos ahora peticiones contra `jsonreader` o contra `jsonproducer` siempre se selecciona la versión `v0` de cada una de ellos (`jsonproducer` siempre devuelve `{"nombre":"manolo"}`

#### Desvío de una fracción del tráfico a una nueva versión

Como comentamos en el apartado <<Control de tráfico y técnicas de despliegue>>, en los despliegues _canary_ se despliega una nueva versión del código, pero sólo se dirige a ella una parte del tráfico con la intención de comenzar a probar el funcionamiento de una nueva versión. Con Istio, este tipo de despliegue lo realizaremos aplicando un nuevo servicio virtual que desviará el 80% del tráfico a la versión `v0` de `jsonproducer` y el 20% del tráfico a la versión `v1` de `jsonproducer`. 

El manifiesto siguiente (https://gist.githubusercontent.com/ualmtorres/7c04cf736f2283fa3c2ffb6bc81a8c30/raw/ae7eb38d0c1d66d08f6792021da15b5ac5462507/virtual-service-jsonproducer-20-v1.yaml[`virtual-service-jsonproducer-20-v1.yaml`,window=_blank]) define un servicio virtual para cada servicio de nuestra aplicación y modifica el original de `jsonproducer` para desviar el 80% del tráfico a `v0` y el 20% a `v1`.

[source, bash]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonreader
spec:
  hosts:
  - jsonreader
  http:
  - route:
    - destination:
        host: jsonreader
        subset: v0
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: jsonproducer
spec:
  hosts:
  - jsonproducer
  http:
  - route:
    - destination: <1>
        host: jsonproducer
        subset: v0
      weight: 80 <2>
    - destination: <3>
        host: jsonproducer
        subset: v1
      weight: 20 <4>
---
----
<1> Configuración de la versión `v0` de `jsonproducer` como destino de tráfico del servicio virtual
<2> Porcentaje de tráfico que se dirigirá a la versión `v0` (80%)
<3> Configuración de la versión `v1` de `jsonproducer` como destino de tráfico del servicio virtual
<4> Porcentaje de tráfico que se dirigirá a la versión `v1` (20%)

Lo aplicaremos en nuestro cluster con 

[source, bash]
----
$ kubectl apply -f https://gist.githubusercontent.com/ualmtorres/7c04cf736f2283fa3c2ffb6bc81a8c30/raw/ae7eb38d0c1d66d08f6792021da15b5ac5462507/virtual-service-jsonproducer-20-v1.yaml
----

Si lanzamos ahora un gran número de peticiones contra `jsonreader` veremos como estadísticamente se tiene a que el 80% de las peticiones muestren los datos de la `v0` de `jsonproducer` y el 20% muestre los datos de la `v1` de `jsonproducer`.

Experimento:

[source, bash]
----
$ ab -n 100 http://greeter-jsonreader.default.192.168.65.94.xip.io/
This is ApacheBench, Version 2.3 <$Revision: 1663405 $>
Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/
Licensed to The Apache Software Foundation, http://www.apache.org/

Benchmarking greeter-jsonreader.default.192.168.65.94.xip.io (be patient).....done


Server Software:        nginx/1.15.6
Server Hostname:        greeter-jsonreader.default.192.168.65.94.xip.io
Server Port:            80

Document Path:          /
Document Length:        11 bytes

Concurrency Level:      1
Time taken for tests:   13.647 seconds
Complete requests:      100
Failed requests:        0
Total transferred:      29965 bytes
HTML transferred:       1100 bytes
Requests per second:    7.33 [#/sec] (mean)
Time per request:       136.466 [ms] (mean)
Time per request:       136.466 [ms] (mean, across all concurrent requests)
Transfer rate:          2.14 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:       46   49   4.6     47      73
Processing:    55   88  37.7     77     268
Waiting:       55   85  37.0     72     268
Total:        101  136  37.5    126     315

Percentage of the requests served within a certain time (ms)
  50%    126
  66%    137
  75%    145
  80%    151
  90%    196
  95%    208
  98%    258
  99%    315
 100%    315 (longest request)
----

Grafo de tráfico en Rancher

image::RancherKialiGraphic.png[]

Métricas de acceso en Rancher

image::RancherKialiMetrics.png[]

Actividad de los servicios en Kiali

image::Kiali.png[]

Visión general de las trazas con Jaeger

image::Jaeger.png[]

Detalles de una traza con Jaeger

image::JaegerTrace.png[]

Actividad del servicio con Grafana

image::Grafana.png[]

Consultas de métricas en Prometheus

image::Prometheus.png[]
